{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6OjwgkXIrHoM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJqFpeAgrSBw"
      },
      "source": [
        "LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xIAyyVJ3rUPo"
      },
      "outputs": [],
      "source": [
        "class WordleLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based neural network for predicting Wordle words.\n",
        "\n",
        "    Architecture rationale:\n",
        "\n",
        "    - Separate Embedding layers:\n",
        "        * Letter embedding converts integer-encoded letters into dense vectors.\n",
        "          This allows the model to learn semantic relationships between letters.\n",
        "        * Feedback embedding converts integer feedback values (0=wrong, 1=wrong position, 2=correct)\n",
        "          into dense vectors, allowing the model to treat feedback as a separate concept.\n",
        "\n",
        "    - LSTM:\n",
        "        * Captures sequential dependencies between consecutive guesses within a Wordle game.\n",
        "        * Later guesses depend on earlier feedback, which the LSTM hidden states can learn.\n",
        "        * Packed sequences are used to efficiently handle variable-length games.\n",
        "\n",
        "    - Dropout layers:\n",
        "        * Applied after LSTM and fully connected layers to reduce overfitting\n",
        "          and improve generalization.\n",
        "\n",
        "    - Fully connected layers:\n",
        "        * Transform LSTM hidden states into predictions for each of the 5 letter positions.\n",
        "        * Output dimension = 5 letters * vocab_size (number of possible letters)\n",
        "        * Softmax applied during training to compute probability distribution over letters.\n",
        "\n",
        "    - Overall:\n",
        "        * The model can predict a 5-letter word at each timestep based on prior guesses and feedback.\n",
        "        * This design separates letter and feedback representations while capturing temporal dependencies.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=29,           # Harf sayısı\n",
        "        letter_embedding_dim=16, # Harf embedding boyutu\n",
        "        feedback_embedding_dim=4,# Feedback embedding boyutu (0,1,2)\n",
        "        hidden_dim=128,\n",
        "        num_layers=2,\n",
        "        dropout=0.3\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Çıkış boyutu: 5 harf * vocab_size\n",
        "        self.output_dim = 5 * vocab_size\n",
        "\n",
        "        # Harf ve feedback embedding\n",
        "        self.letter_embedding = nn.Embedding(vocab_size, letter_embedding_dim)\n",
        "        self.feedback_embedding = nn.Embedding(3, feedback_embedding_dim)\n",
        "\n",
        "        # LSTM input boyutu = 5 harf * letter_emb + 5 feedback * feedback_emb\n",
        "        self.lstm_input_dim = 5 * letter_embedding_dim + 5 * feedback_embedding_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.post_lstm_dropout = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, self.output_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len, 10) -> 5 harf + 5 feedback\n",
        "        lengths: (batch_size,) sequence uzunlukları\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Harf ve feedback ayrımı\n",
        "        letters = x[:, :, :5]   # pg1..pg5\n",
        "        feedback = x[:, :, 5:]  # l1..l5\n",
        "\n",
        "        # Embedding\n",
        "        letters_emb = self.letter_embedding(letters)       # (B, S, 5, letter_embedding_dim)\n",
        "        feedback_emb = self.feedback_embedding(feedback)   # (B, S, 5, feedback_embedding_dim)\n",
        "\n",
        "        # Concatenate embeddingler\n",
        "        x_embed = torch.cat([letters_emb, feedback_emb], dim=-1)  # (B, S, 5, letter+fb emb)\n",
        "        x_embed = x_embed.view(batch_size, seq_len, -1)            # (B, S, LSTM_input_dim)\n",
        "\n",
        "        # Packed sequence ile LSTM\n",
        "        packed_x = nn.utils.rnn.pack_padded_sequence(\n",
        "            x_embed,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        packed_out, (h, c) = self.lstm(packed_x)\n",
        "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        # Dropout + FC\n",
        "        out = self.post_lstm_dropout(out)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.post_lstm_dropout(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # Çıkış reshape: (B, S, 5 harf, vocab_size)\n",
        "        out = out.view(batch_size, seq_len, 5, -1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvNETa0rsKp"
      },
      "source": [
        "TRAIN AND TEST FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1We-K-VErpNy"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Trains the given model for one epoch using the provided DataLoader.\n",
        "\n",
        "    This function performs a full training loop:\n",
        "    - Iterates over all batches in the DataLoader\n",
        "    - Moves data to the target device (CPU/GPU)\n",
        "    - Computes model predictions (logits)\n",
        "    - Calculates loss and backpropagates gradients\n",
        "    - Updates model weights using the optimizer\n",
        "    - Tracks average loss and overall accuracy\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained\n",
        "        dataloader (torch.utils.data.DataLoader): Provides training batches\n",
        "        optimizer (torch.optim.Optimizer): Optimization algorithm (e.g., Adam, SGD)\n",
        "        criterion (torch.nn.Module): Loss function (e.g., CrossEntropyLoss)\n",
        "        device (torch.device): Target device (\"cpu\" or \"cuda\")\n",
        "\n",
        "    Returns:\n",
        "        avg_loss (float): Average loss over all batches\n",
        "        accuracy (float): Overall prediction accuracy (in %)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for X, Y, lengths in dataloader:\n",
        "        X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(X, lengths)\n",
        "        logits_flat = logits.view(-1, 29)\n",
        "        Y_flat = Y.view(-1)\n",
        "\n",
        "        loss = criterion(logits_flat, Y_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits_flat, dim=1)\n",
        "        correct_predictions += (predicted == Y_flat).sum().item()\n",
        "        total_predictions += Y_flat.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100.0 * correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YVvtwRaxrrdg"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates the given model on a validation or test dataset.\n",
        "\n",
        "    This function performs an evaluation loop:\n",
        "    - Sets the model to evaluation mode (disables dropout, batchnorm updates)\n",
        "    - Iterates over all batches in the DataLoader without computing gradients\n",
        "    - Computes model predictions (logits)\n",
        "    - Calculates loss and overall accuracy\n",
        "    - Returns average loss and accuracy across the dataset\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be evaluated\n",
        "        dataloader (torch.utils.data.DataLoader): Provides test/validation batches\n",
        "        criterion (torch.nn.Module): Loss function (e.g., CrossEntropyLoss)\n",
        "        device (torch.device): Target device (\"cpu\" or \"cuda\")\n",
        "\n",
        "    Returns:\n",
        "        avg_loss (float): Average loss over all batches\n",
        "        accuracy (float): Overall prediction accuracy (in %)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, Y, lengths in dataloader:\n",
        "            X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
        "\n",
        "            logits = model(X, lengths)\n",
        "            logits_flat = logits.view(-1, 29)\n",
        "            Y_flat = Y.view(-1)\n",
        "\n",
        "            loss = criterion(logits_flat, Y_flat)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(logits_flat, dim=1)\n",
        "            correct_predictions += (predicted == Y_flat).sum().item()\n",
        "            total_predictions += Y_flat.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = 100.0 * correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pczJBhSwrzoU"
      },
      "source": [
        "DATASET PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LaAjeQWjsIYu"
      },
      "outputs": [],
      "source": [
        "class WordleSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset for handling Wordle gameplay data in sequence format.\n",
        "    - Each game (identified by 'gameid') contains multiple attempts (rows).\n",
        "    - Each attempt has encoded features for guesses and feedback.\n",
        "    - The dataset groups and converts them into sequential tensors for model training.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_frame):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequences = []\n",
        "        self.labels = []\n",
        "        self.lengths = []\n",
        "        # Group all attempts by game ID (each game is treated as one sequence)\n",
        "        grouped = data_frame.groupby(\"gameid\")\n",
        "        # Process each game sequence separately\n",
        "        for gameid, group in grouped:\n",
        "            group_sorted = group.sort_values(by=\"attempt_index\")\n",
        "\n",
        "            # Input vector X:\n",
        "            # - pg1..pg5: previous guessed letters\n",
        "            # - l1..l5: feedback for each letter (e.g., correct/wrong position)\n",
        "            # Combined feature dimension = 10 per timestep\n",
        "            X = group_sorted[[\n",
        "                \"pg1\", \"pg2\", \"pg3\", \"pg4\", \"pg5\",\n",
        "                \"l1\", \"l2\", \"l3\", \"l4\", \"l5\"\n",
        "                        ]].values\n",
        "\n",
        "            # Target vector Y:\n",
        "            # - t1..t5: target word (true letters encoded as integers)\n",
        "            Y = group_sorted[[\"t1\", \"t2\", \"t3\", \"t4\", \"t5\"]].values\n",
        "\n",
        "            X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "            Y_tensor = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "            self.sequences.append(X_tensor)\n",
        "            self.labels.append(Y_tensor)\n",
        "            self.lengths.append(X_tensor.size(0))\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of games in the dataset\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return one game sequence (X, Y, length)\n",
        "        return self.sequences[idx], self.labels[idx], self.lengths[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader.\n",
        "    Pads sequences of different lengths so they can form uniform batches.\n",
        "    Each batch is a list of tuples: (X_seq, Y_seq, seq_len)\n",
        "    \"\"\"\n",
        "    sequences = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    lengths = [item[2] for item in batch]\n",
        "    # Pad sequences with zeros to match the length of the longest sequence in the batch\n",
        "    # This allows variable-length games to be trained together efficiently\n",
        "    padded_X = pad_sequence(sequences, batch_first=True)\n",
        "    padded_Y = pad_sequence(labels, batch_first=True)\n",
        "    # Convert lengths to a tensor (used later for masking or packing)\n",
        "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return padded_X, padded_Y, lengths_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "save_dir = \"/content/drive/MyDrive/colab_kfold_results\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhe6s9Wo_uYd",
        "outputId": "dca1d2ae-3d89-4590-80c7-69d75f7c9596"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xnZMs2s5g1_O"
      },
      "outputs": [],
      "source": [
        "def kfold_training_save_report(df, model_class, k=5,lr=0.001, batch_size=16, num_epochs=10, device=\"cpu\",\n",
        "                               random_state=42, save_path=\"best_model.pth\", report_path=\"training_report.csv\"):\n",
        "    \"\"\"\n",
        "    Performs K-Fold Cross-Validation on Wordle dataset, saves the best model,\n",
        "    and writes training report to CSV.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Full dataset with 'gameid' column.\n",
        "        model_class: Model class (e.g., WordleLSTM).\n",
        "        k (int): Number of folds.\n",
        "        batch_size (int): Batch size for DataLoader.\n",
        "        num_epochs (int): Training epochs per fold.\n",
        "        device (str): \"cpu\" or \"cuda\".\n",
        "        random_state (int): Random seed for reproducibility.\n",
        "        save_path (str): File path to save the best model.\n",
        "        report_path (str): File path to save training report CSV.\n",
        "\n",
        "    Returns:\n",
        "        fold_results (list of tuples): (train_loss, train_acc, test_loss, test_acc) for each fold.\n",
        "    \"\"\"\n",
        "\n",
        "    all_gameids = df['gameid'].unique()\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
        "    fold_results = []\n",
        "\n",
        "    best_test_acc = 0.0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Records for CSV\n",
        "    records = []\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(kf.split(all_gameids)):\n",
        "        print(f\"\\n=== Fold {fold+1}/{k} ===\")\n",
        "\n",
        "        # ID setlerini oluştur\n",
        "        train_ids = set(all_gameids[train_index])\n",
        "        test_ids = set(all_gameids[test_index])\n",
        "\n",
        "        # Mask ile filtreleme\n",
        "        train_mask = df['gameid'].map(lambda x: x in train_ids)\n",
        "        test_mask = df['gameid'].map(lambda x: x in test_ids)\n",
        "        train_df = df[train_mask]\n",
        "        test_df = df[test_mask]\n",
        "\n",
        "        train_dataset = WordleSequenceDataset(train_df)\n",
        "        test_dataset = WordleSequenceDataset(test_df)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        model = model_class(\n",
        "                    vocab_size=29,      # letter numbers in alphabet\n",
        "                    letter_embedding_dim=16,    # Previous Guess Embedding dimension\n",
        "                    feedback_embedding_dim=4,   # Feedback Embedding dimension\n",
        "                    hidden_dim=256,     # LSTM hidden dimension\n",
        "                    num_layers=4,       # LSTM layer number\n",
        "                    dropout=0.3         # Drop out\n",
        "                ).to(device)\n",
        "\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Fold {fold+1}, Epoch {epoch+1}: \")\n",
        "            train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "            test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "\n",
        "            print(f\"Train Loss {train_loss:.4f}, Train Acc {train_acc:.2f}%, \"\n",
        "                  f\"Test Loss {test_loss:.4f}, Test Acc {test_acc:.2f}%\")\n",
        "\n",
        "            # Save metrics for CSV\n",
        "            records.append({\n",
        "                \"fold\": fold+1,\n",
        "                \"epoch\": epoch+1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_acc\": train_acc,\n",
        "                \"test_loss\": test_loss,\n",
        "                \"test_acc\": test_acc\n",
        "            })\n",
        "\n",
        "\n",
        "            # Save best model\n",
        "            if test_acc > best_test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                best_model_state = model.state_dict()\n",
        "\n",
        "\n",
        "        fold_results.append((train_loss, train_acc, test_loss, test_acc))\n",
        "\n",
        "        # CSV’ye ekle (append)\n",
        "        df_report = pd.DataFrame(records)\n",
        "        df_report.to_csv(os.path.join(save_dir,report_path), mode='a', index=False, header=False)\n",
        "         # en iyi modeli kaydet\n",
        "        model_path = os.path.join(save_dir, save_path)\n",
        "        torch.save(best_model_state, model_path)\n",
        "\n",
        "    print(f\"\\nTraining report saved at: {report_path}\")\n",
        "    print(f\"Best Test Accuracy across folds: {best_test_acc:.2f}%\")\n",
        "    print(f\"Best model saved at: {save_path}\")\n",
        "\n",
        "    return fold_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IDVTVRmqr8kx"
      },
      "outputs": [],
      "source": [
        "def main(csv_path=\"\",\n",
        "         epochs=1,\n",
        "         batch_size=1,\n",
        "         lr=1e-3,\n",
        "         shuffle=True):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Running on: {device}\")\n",
        "\n",
        "\n",
        "    df = pd.read_csv(csv_path,header=None)\n",
        "    df.columns = [\n",
        "    \"gameid\",\n",
        "    \"attempt_index\",\n",
        "    \"pg1\", \"pg2\", \"pg3\", \"pg4\", \"pg5\",\n",
        "    \"l1\", \"l2\", \"l3\", \"l4\", \"l5\",\n",
        "    \"g1\", \"g2\", \"g3\", \"g4\", \"g5\",\n",
        "    \"t1\", \"t2\", \"t3\", \"t4\", \"t5\"\n",
        "    ]\n",
        "\n",
        "    # Kontrol\n",
        "    print(df.head())\n",
        "    results = kfold_training_save_report(\n",
        "    df=df,\n",
        "    model_class=WordleLSTM,\n",
        "    k=5,lr=lr,\n",
        "    batch_size=batch_size,\n",
        "    num_epochs=epochs,\n",
        "    device=device,\n",
        "    save_path=\"best_wordle_model.pth\",\n",
        "    report_path=\"training_report.csv\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yFSH1-e41s6",
        "outputId": "3e95a8ee-c839-499a-bda0-fff464c9c7a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on: cuda\n",
            "   gameid  attempt_index  pg1  pg2  pg3  pg4  pg5  l1  l2  l3  ...  g1  g2  \\\n",
            "0       0              0    1   24   27   17   23   0   0   0  ...   9  17   \n",
            "1       0              1    9   17   19   19    0   0   2   0  ...   2  17   \n",
            "2       0              2    2   17   22   15    0   0   2   0  ...  14  17   \n",
            "3       0              3   14   17   16    7    0   0   2   0  ...   6  17   \n",
            "4       0              4    6   17    4   20    0   2   2   0  ...   6  17   \n",
            "\n",
            "   g3  g4  g5  t1  t2  t3  t4  t5  \n",
            "0  19  19   0   6  17  20  28   0  \n",
            "1  22  15   0   6  17  20  28   0  \n",
            "2  16   7   0   6  17  20  28   0  \n",
            "3   4  20   0   6  17  20  28   0  \n",
            "4  20  28   0   6  17  20  28   0  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "=== Fold 1/5 ===\n",
            "Fold 1, Epoch 1: \n",
            "Train Loss 1.7899, Train Acc 56.54%, Test Loss 1.2978, Test Acc 61.76%\n",
            "Fold 1, Epoch 2: \n",
            "Train Loss 1.2421, Train Acc 63.15%, Test Loss 1.1635, Test Acc 65.19%\n",
            "Fold 1, Epoch 3: \n",
            "Train Loss 1.1649, Train Acc 65.18%, Test Loss 1.1140, Test Acc 66.52%\n",
            "Fold 1, Epoch 4: \n",
            "Train Loss 1.1253, Train Acc 66.29%, Test Loss 1.0789, Test Acc 67.65%\n",
            "Fold 1, Epoch 5: \n",
            "Train Loss 1.0954, Train Acc 67.27%, Test Loss 1.0491, Test Acc 68.61%\n",
            "Fold 1, Epoch 6: \n",
            "Train Loss 1.0709, Train Acc 68.05%, Test Loss 1.0196, Test Acc 69.43%\n",
            "Fold 1, Epoch 7: \n",
            "Train Loss 1.0455, Train Acc 68.81%, Test Loss 0.9928, Test Acc 70.40%\n",
            "Fold 1, Epoch 8: \n",
            "Train Loss 1.0237, Train Acc 69.56%, Test Loss 0.9680, Test Acc 71.16%\n",
            "Fold 1, Epoch 9: \n",
            "Train Loss 1.0032, Train Acc 70.22%, Test Loss 0.9470, Test Acc 71.81%\n",
            "Fold 1, Epoch 10: \n",
            "Train Loss 0.9862, Train Acc 70.75%, Test Loss 0.9298, Test Acc 72.32%\n",
            "Fold 1, Epoch 11: \n",
            "Train Loss 0.9724, Train Acc 71.18%, Test Loss 0.9154, Test Acc 72.77%\n",
            "Fold 1, Epoch 12: \n",
            "Train Loss 0.9594, Train Acc 71.58%, Test Loss 0.9018, Test Acc 73.15%\n",
            "Fold 1, Epoch 13: \n",
            "Train Loss 0.9487, Train Acc 71.90%, Test Loss 0.8904, Test Acc 73.49%\n",
            "Fold 1, Epoch 14: \n",
            "Train Loss 0.9393, Train Acc 72.19%, Test Loss 0.8800, Test Acc 73.77%\n",
            "Fold 1, Epoch 15: \n",
            "Train Loss 0.9306, Train Acc 72.45%, Test Loss 0.8710, Test Acc 74.03%\n",
            "Fold 1, Epoch 16: \n",
            "Train Loss 0.9227, Train Acc 72.69%, Test Loss 0.8631, Test Acc 74.23%\n",
            "Fold 1, Epoch 17: \n",
            "Train Loss 0.9158, Train Acc 72.88%, Test Loss 0.8565, Test Acc 74.41%\n",
            "Fold 1, Epoch 18: \n",
            "Train Loss 0.9094, Train Acc 73.06%, Test Loss 0.8503, Test Acc 74.55%\n",
            "Fold 1, Epoch 19: \n",
            "Train Loss 0.9040, Train Acc 73.22%, Test Loss 0.8451, Test Acc 74.70%\n",
            "Fold 1, Epoch 20: \n",
            "Train Loss 0.8997, Train Acc 73.33%, Test Loss 0.8404, Test Acc 74.81%\n",
            "Fold 1, Epoch 21: \n",
            "Train Loss 0.8948, Train Acc 73.46%, Test Loss 0.8360, Test Acc 74.94%\n",
            "Fold 1, Epoch 22: \n",
            "Train Loss 0.8908, Train Acc 73.57%, Test Loss 0.8321, Test Acc 75.03%\n",
            "Fold 1, Epoch 23: \n",
            "Train Loss 0.8870, Train Acc 73.67%, Test Loss 0.8285, Test Acc 75.13%\n",
            "Fold 1, Epoch 24: \n",
            "Train Loss 0.8841, Train Acc 73.74%, Test Loss 0.8249, Test Acc 75.21%\n",
            "Fold 1, Epoch 25: \n",
            "Train Loss 0.8808, Train Acc 73.83%, Test Loss 0.8215, Test Acc 75.30%\n",
            "Fold 1, Epoch 26: \n",
            "Train Loss 0.8772, Train Acc 73.92%, Test Loss 0.8179, Test Acc 75.37%\n",
            "Fold 1, Epoch 27: \n",
            "Train Loss 0.8748, Train Acc 73.98%, Test Loss 0.8149, Test Acc 75.43%\n",
            "Fold 1, Epoch 28: \n",
            "Train Loss 0.8712, Train Acc 74.06%, Test Loss 0.8115, Test Acc 75.50%\n",
            "Fold 1, Epoch 29: \n",
            "Train Loss 0.8691, Train Acc 74.11%, Test Loss 0.8088, Test Acc 75.57%\n",
            "Fold 1, Epoch 30: \n",
            "Train Loss 0.8659, Train Acc 74.18%, Test Loss 0.8065, Test Acc 75.61%\n",
            "Fold 1, Epoch 31: \n",
            "Train Loss 0.8638, Train Acc 74.24%, Test Loss 0.8041, Test Acc 75.66%\n",
            "Fold 1, Epoch 32: \n",
            "Train Loss 0.8609, Train Acc 74.30%, Test Loss 0.8020, Test Acc 75.71%\n",
            "Fold 1, Epoch 33: \n",
            "Train Loss 0.8582, Train Acc 74.37%, Test Loss 0.8000, Test Acc 75.75%\n",
            "Fold 1, Epoch 34: \n",
            "Train Loss 0.8565, Train Acc 74.41%, Test Loss 0.7978, Test Acc 75.81%\n",
            "Fold 1, Epoch 35: \n",
            "Train Loss 0.8543, Train Acc 74.47%, Test Loss 0.7959, Test Acc 75.85%\n",
            "Fold 1, Epoch 36: \n",
            "Train Loss 0.8527, Train Acc 74.50%, Test Loss 0.7941, Test Acc 75.90%\n",
            "Fold 1, Epoch 37: \n",
            "Train Loss 0.8507, Train Acc 74.55%, Test Loss 0.7920, Test Acc 75.93%\n",
            "Fold 1, Epoch 38: \n",
            "Train Loss 0.8488, Train Acc 74.58%, Test Loss 0.7902, Test Acc 75.96%\n",
            "Fold 1, Epoch 39: \n",
            "Train Loss 0.8476, Train Acc 74.62%, Test Loss 0.7889, Test Acc 76.00%\n",
            "Fold 1, Epoch 40: \n",
            "Train Loss 0.8453, Train Acc 74.67%, Test Loss 0.7870, Test Acc 76.03%\n",
            "Fold 1, Epoch 41: \n",
            "Train Loss 0.8438, Train Acc 74.70%, Test Loss 0.7854, Test Acc 76.07%\n",
            "Fold 1, Epoch 42: \n",
            "Train Loss 0.8422, Train Acc 74.74%, Test Loss 0.7837, Test Acc 76.10%\n",
            "Fold 1, Epoch 43: \n",
            "Train Loss 0.8405, Train Acc 74.78%, Test Loss 0.7823, Test Acc 76.13%\n",
            "Fold 1, Epoch 44: \n",
            "Train Loss 0.8391, Train Acc 74.80%, Test Loss 0.7809, Test Acc 76.16%\n",
            "Fold 1, Epoch 45: \n",
            "Train Loss 0.8379, Train Acc 74.83%, Test Loss 0.7791, Test Acc 76.19%\n",
            "Fold 1, Epoch 46: \n",
            "Train Loss 0.8365, Train Acc 74.86%, Test Loss 0.7777, Test Acc 76.22%\n",
            "Fold 1, Epoch 47: \n",
            "Train Loss 0.8343, Train Acc 74.91%, Test Loss 0.7764, Test Acc 76.24%\n",
            "Fold 1, Epoch 48: \n",
            "Train Loss 0.8332, Train Acc 74.92%, Test Loss 0.7748, Test Acc 76.26%\n",
            "Fold 1, Epoch 49: \n",
            "Train Loss 0.8318, Train Acc 74.95%, Test Loss 0.7729, Test Acc 76.30%\n",
            "Fold 1, Epoch 50: \n",
            "Train Loss 0.8304, Train Acc 74.98%, Test Loss 0.7713, Test Acc 76.33%\n",
            "Fold 1, Epoch 51: \n",
            "Train Loss 0.8293, Train Acc 74.99%, Test Loss 0.7699, Test Acc 76.35%\n",
            "Fold 1, Epoch 52: \n",
            "Train Loss 0.8279, Train Acc 75.02%, Test Loss 0.7684, Test Acc 76.38%\n",
            "Fold 1, Epoch 53: \n",
            "Train Loss 0.8265, Train Acc 75.05%, Test Loss 0.7673, Test Acc 76.40%\n",
            "Fold 1, Epoch 54: \n",
            "Train Loss 0.8253, Train Acc 75.07%, Test Loss 0.7661, Test Acc 76.42%\n",
            "Fold 1, Epoch 55: \n",
            "Train Loss 0.8241, Train Acc 75.09%, Test Loss 0.7650, Test Acc 76.44%\n",
            "Fold 1, Epoch 56: \n",
            "Train Loss 0.8233, Train Acc 75.10%, Test Loss 0.7641, Test Acc 76.45%\n",
            "Fold 1, Epoch 57: \n",
            "Train Loss 0.8220, Train Acc 75.13%, Test Loss 0.7630, Test Acc 76.48%\n",
            "Fold 1, Epoch 58: \n",
            "Train Loss 0.8214, Train Acc 75.15%, Test Loss 0.7621, Test Acc 76.49%\n",
            "Fold 1, Epoch 59: \n",
            "Train Loss 0.8199, Train Acc 75.18%, Test Loss 0.7612, Test Acc 76.52%\n",
            "Fold 1, Epoch 60: \n",
            "Train Loss 0.8196, Train Acc 75.19%, Test Loss 0.7601, Test Acc 76.53%\n",
            "Fold 1, Epoch 61: \n",
            "Train Loss 0.8187, Train Acc 75.20%, Test Loss 0.7595, Test Acc 76.55%\n",
            "Fold 1, Epoch 62: \n",
            "Train Loss 0.8180, Train Acc 75.21%, Test Loss 0.7586, Test Acc 76.57%\n",
            "Fold 1, Epoch 63: \n",
            "Train Loss 0.8168, Train Acc 75.24%, Test Loss 0.7575, Test Acc 76.59%\n",
            "Fold 1, Epoch 64: \n",
            "Train Loss 0.8159, Train Acc 75.25%, Test Loss 0.7570, Test Acc 76.59%\n",
            "Fold 1, Epoch 65: \n",
            "Train Loss 0.8149, Train Acc 75.27%, Test Loss 0.7562, Test Acc 76.62%\n",
            "Fold 1, Epoch 66: \n",
            "Train Loss 0.8139, Train Acc 75.30%, Test Loss 0.7555, Test Acc 76.62%\n",
            "Fold 1, Epoch 67: \n",
            "Train Loss 0.8132, Train Acc 75.31%, Test Loss 0.7549, Test Acc 76.62%\n",
            "Fold 1, Epoch 68: \n",
            "Train Loss 0.8126, Train Acc 75.32%, Test Loss 0.7543, Test Acc 76.64%\n",
            "Fold 1, Epoch 69: \n",
            "Train Loss 0.8117, Train Acc 75.34%, Test Loss 0.7536, Test Acc 76.66%\n",
            "Fold 1, Epoch 70: \n",
            "Train Loss 0.8106, Train Acc 75.37%, Test Loss 0.7531, Test Acc 76.67%\n",
            "Fold 1, Epoch 71: \n",
            "Train Loss 0.8103, Train Acc 75.37%, Test Loss 0.7525, Test Acc 76.68%\n",
            "Fold 1, Epoch 72: \n",
            "Train Loss 0.8090, Train Acc 75.40%, Test Loss 0.7521, Test Acc 76.68%\n",
            "Fold 1, Epoch 73: \n",
            "Train Loss 0.8089, Train Acc 75.40%, Test Loss 0.7514, Test Acc 76.70%\n",
            "Fold 1, Epoch 74: \n",
            "Train Loss 0.8082, Train Acc 75.42%, Test Loss 0.7511, Test Acc 76.71%\n",
            "Fold 1, Epoch 75: \n",
            "Train Loss 0.8075, Train Acc 75.44%, Test Loss 0.7506, Test Acc 76.72%\n",
            "Fold 1, Epoch 76: \n",
            "Train Loss 0.8069, Train Acc 75.45%, Test Loss 0.7501, Test Acc 76.73%\n",
            "Fold 1, Epoch 77: \n",
            "Train Loss 0.8067, Train Acc 75.45%, Test Loss 0.7496, Test Acc 76.75%\n",
            "Fold 1, Epoch 78: \n",
            "Train Loss 0.8061, Train Acc 75.46%, Test Loss 0.7492, Test Acc 76.74%\n",
            "Fold 1, Epoch 79: \n",
            "Train Loss 0.8054, Train Acc 75.48%, Test Loss 0.7489, Test Acc 76.76%\n",
            "Fold 1, Epoch 80: \n",
            "Train Loss 0.8051, Train Acc 75.49%, Test Loss 0.7484, Test Acc 76.77%\n",
            "Fold 1, Epoch 81: \n",
            "Train Loss 0.8045, Train Acc 75.50%, Test Loss 0.7481, Test Acc 76.77%\n",
            "Fold 1, Epoch 82: \n",
            "Train Loss 0.8043, Train Acc 75.51%, Test Loss 0.7475, Test Acc 76.79%\n",
            "Fold 1, Epoch 83: \n",
            "Train Loss 0.8038, Train Acc 75.52%, Test Loss 0.7474, Test Acc 76.79%\n",
            "Fold 1, Epoch 84: \n",
            "Train Loss 0.8034, Train Acc 75.53%, Test Loss 0.7467, Test Acc 76.80%\n",
            "Fold 1, Epoch 85: \n",
            "Train Loss 0.8028, Train Acc 75.54%, Test Loss 0.7465, Test Acc 76.81%\n",
            "Fold 1, Epoch 86: \n",
            "Train Loss 0.8029, Train Acc 75.54%, Test Loss 0.7461, Test Acc 76.82%\n",
            "Fold 1, Epoch 87: \n",
            "Train Loss 0.8018, Train Acc 75.56%, Test Loss 0.7457, Test Acc 76.82%\n",
            "Fold 1, Epoch 88: \n",
            "Train Loss 0.8015, Train Acc 75.56%, Test Loss 0.7454, Test Acc 76.84%\n",
            "Fold 1, Epoch 89: \n",
            "Train Loss 0.8009, Train Acc 75.59%, Test Loss 0.7451, Test Acc 76.84%\n",
            "Fold 1, Epoch 90: \n",
            "Train Loss 0.8004, Train Acc 75.60%, Test Loss 0.7449, Test Acc 76.85%\n",
            "Fold 1, Epoch 91: \n",
            "Train Loss 0.8006, Train Acc 75.60%, Test Loss 0.7445, Test Acc 76.86%\n",
            "Fold 1, Epoch 92: \n",
            "Train Loss 0.7998, Train Acc 75.62%, Test Loss 0.7442, Test Acc 76.88%\n",
            "Fold 1, Epoch 93: \n",
            "Train Loss 0.7997, Train Acc 75.61%, Test Loss 0.7438, Test Acc 76.87%\n",
            "Fold 1, Epoch 94: \n",
            "Train Loss 0.7992, Train Acc 75.63%, Test Loss 0.7434, Test Acc 76.88%\n",
            "Fold 1, Epoch 95: \n",
            "Train Loss 0.7993, Train Acc 75.62%, Test Loss 0.7432, Test Acc 76.89%\n",
            "Fold 1, Epoch 96: \n",
            "Train Loss 0.7987, Train Acc 75.64%, Test Loss 0.7429, Test Acc 76.89%\n",
            "Fold 1, Epoch 97: \n",
            "Train Loss 0.7984, Train Acc 75.65%, Test Loss 0.7426, Test Acc 76.90%\n",
            "Fold 1, Epoch 98: \n",
            "Train Loss 0.7979, Train Acc 75.66%, Test Loss 0.7423, Test Acc 76.91%\n",
            "Fold 1, Epoch 99: \n",
            "Train Loss 0.7976, Train Acc 75.66%, Test Loss 0.7419, Test Acc 76.91%\n",
            "Fold 1, Epoch 100: \n",
            "Train Loss 0.7971, Train Acc 75.68%, Test Loss 0.7418, Test Acc 76.92%\n",
            "\n",
            "=== Fold 2/5 ===\n",
            "Fold 2, Epoch 1: \n",
            "Train Loss 1.8029, Train Acc 55.92%, Test Loss 1.3501, Test Acc 60.66%\n",
            "Fold 2, Epoch 2: \n",
            "Train Loss 1.2782, Train Acc 62.33%, Test Loss 1.1847, Test Acc 64.74%\n",
            "Fold 2, Epoch 3: \n",
            "Train Loss 1.1782, Train Acc 64.90%, Test Loss 1.1182, Test Acc 66.41%\n",
            "Fold 2, Epoch 4: \n",
            "Train Loss 1.1292, Train Acc 66.26%, Test Loss 1.0810, Test Acc 67.58%\n",
            "Fold 2, Epoch 5: \n",
            "Train Loss 1.0993, Train Acc 67.20%, Test Loss 1.0508, Test Acc 68.48%\n",
            "Fold 2, Epoch 6: \n",
            "Train Loss 1.0737, Train Acc 67.95%, Test Loss 1.0223, Test Acc 69.32%\n",
            "Fold 2, Epoch 7: \n",
            "Train Loss 1.0484, Train Acc 68.72%, Test Loss 0.9949, Test Acc 70.25%\n",
            "Fold 2, Epoch 8: \n",
            "Train Loss 1.0258, Train Acc 69.47%, Test Loss 0.9720, Test Acc 70.93%\n",
            "Fold 2, Epoch 9: \n",
            "Train Loss 1.0073, Train Acc 70.04%, Test Loss 0.9535, Test Acc 71.45%\n",
            "Fold 2, Epoch 10: \n",
            "Train Loss 0.9915, Train Acc 70.51%, Test Loss 0.9384, Test Acc 71.89%\n",
            "Fold 2, Epoch 11: \n",
            "Train Loss 0.9787, Train Acc 70.90%, Test Loss 0.9244, Test Acc 72.34%\n",
            "Fold 2, Epoch 12: \n",
            "Train Loss 0.9659, Train Acc 71.29%, Test Loss 0.9111, Test Acc 72.74%\n",
            "Fold 2, Epoch 13: \n",
            "Train Loss 0.9552, Train Acc 71.62%, Test Loss 0.9004, Test Acc 73.08%\n",
            "Fold 2, Epoch 14: \n",
            "Train Loss 0.9452, Train Acc 71.93%, Test Loss 0.8902, Test Acc 73.39%\n",
            "Fold 2, Epoch 15: \n",
            "Train Loss 0.9367, Train Acc 72.20%, Test Loss 0.8809, Test Acc 73.64%\n",
            "Fold 2, Epoch 16: \n",
            "Train Loss 0.9285, Train Acc 72.45%, Test Loss 0.8722, Test Acc 73.91%\n",
            "Fold 2, Epoch 17: \n",
            "Train Loss 0.9212, Train Acc 72.67%, Test Loss 0.8650, Test Acc 74.09%\n",
            "Fold 2, Epoch 18: \n",
            "Train Loss 0.9148, Train Acc 72.86%, Test Loss 0.8573, Test Acc 74.30%\n",
            "Fold 2, Epoch 19: \n",
            "Train Loss 0.9087, Train Acc 73.03%, Test Loss 0.8513, Test Acc 74.47%\n",
            "Fold 2, Epoch 20: \n",
            "Train Loss 0.9030, Train Acc 73.19%, Test Loss 0.8454, Test Acc 74.61%\n",
            "Fold 2, Epoch 21: \n",
            "Train Loss 0.8981, Train Acc 73.32%, Test Loss 0.8405, Test Acc 74.74%\n",
            "Fold 2, Epoch 22: \n",
            "Train Loss 0.8928, Train Acc 73.46%, Test Loss 0.8346, Test Acc 74.87%\n",
            "Fold 2, Epoch 23: \n",
            "Train Loss 0.8883, Train Acc 73.59%, Test Loss 0.8299, Test Acc 74.98%\n",
            "Fold 2, Epoch 24: \n",
            "Train Loss 0.8850, Train Acc 73.67%, Test Loss 0.8257, Test Acc 75.08%\n",
            "Fold 2, Epoch 25: \n",
            "Train Loss 0.8812, Train Acc 73.76%, Test Loss 0.8220, Test Acc 75.18%\n",
            "Fold 2, Epoch 26: \n",
            "Train Loss 0.8770, Train Acc 73.87%, Test Loss 0.8183, Test Acc 75.26%\n",
            "Fold 2, Epoch 27: \n",
            "Train Loss 0.8738, Train Acc 73.95%, Test Loss 0.8151, Test Acc 75.33%\n",
            "Fold 2, Epoch 28: \n",
            "Train Loss 0.8709, Train Acc 74.02%, Test Loss 0.8121, Test Acc 75.39%\n",
            "Fold 2, Epoch 29: \n",
            "Train Loss 0.8676, Train Acc 74.11%, Test Loss 0.8095, Test Acc 75.45%\n",
            "Fold 2, Epoch 30: \n",
            "Train Loss 0.8653, Train Acc 74.16%, Test Loss 0.8068, Test Acc 75.51%\n",
            "Fold 2, Epoch 31: \n",
            "Train Loss 0.8623, Train Acc 74.23%, Test Loss 0.8048, Test Acc 75.56%\n",
            "Fold 2, Epoch 32: \n",
            "Train Loss 0.8610, Train Acc 74.26%, Test Loss 0.8022, Test Acc 75.61%\n",
            "Fold 2, Epoch 33: \n",
            "Train Loss 0.8579, Train Acc 74.34%, Test Loss 0.8000, Test Acc 75.66%\n",
            "Fold 2, Epoch 34: \n",
            "Train Loss 0.8557, Train Acc 74.39%, Test Loss 0.7979, Test Acc 75.71%\n",
            "Fold 2, Epoch 35: \n",
            "Train Loss 0.8535, Train Acc 74.43%, Test Loss 0.7958, Test Acc 75.76%\n",
            "Fold 2, Epoch 36: \n",
            "Train Loss 0.8516, Train Acc 74.48%, Test Loss 0.7939, Test Acc 75.80%\n",
            "Fold 2, Epoch 37: \n",
            "Train Loss 0.8495, Train Acc 74.53%, Test Loss 0.7921, Test Acc 75.83%\n",
            "Fold 2, Epoch 38: \n",
            "Train Loss 0.8477, Train Acc 74.57%, Test Loss 0.7906, Test Acc 75.86%\n",
            "Fold 2, Epoch 39: \n",
            "Train Loss 0.8461, Train Acc 74.61%, Test Loss 0.7890, Test Acc 75.90%\n",
            "Fold 2, Epoch 40: \n",
            "Train Loss 0.8442, Train Acc 74.65%, Test Loss 0.7872, Test Acc 75.94%\n",
            "Fold 2, Epoch 41: \n",
            "Train Loss 0.8428, Train Acc 74.68%, Test Loss 0.7858, Test Acc 75.97%\n",
            "Fold 2, Epoch 42: \n",
            "Train Loss 0.8407, Train Acc 74.72%, Test Loss 0.7839, Test Acc 76.01%\n",
            "Fold 2, Epoch 43: \n",
            "Train Loss 0.8392, Train Acc 74.76%, Test Loss 0.7823, Test Acc 76.05%\n",
            "Fold 2, Epoch 44: \n",
            "Train Loss 0.8387, Train Acc 74.77%, Test Loss 0.7804, Test Acc 76.08%\n",
            "Fold 2, Epoch 45: \n",
            "Train Loss 0.8367, Train Acc 74.81%, Test Loss 0.7788, Test Acc 76.11%\n",
            "Fold 2, Epoch 46: \n",
            "Train Loss 0.8347, Train Acc 74.84%, Test Loss 0.7770, Test Acc 76.14%\n",
            "Fold 2, Epoch 47: \n",
            "Train Loss 0.8336, Train Acc 74.86%, Test Loss 0.7752, Test Acc 76.17%\n",
            "Fold 2, Epoch 48: \n",
            "Train Loss 0.8319, Train Acc 74.90%, Test Loss 0.7736, Test Acc 76.20%\n",
            "Fold 2, Epoch 49: \n",
            "Train Loss 0.8302, Train Acc 74.92%, Test Loss 0.7719, Test Acc 76.22%\n",
            "Fold 2, Epoch 50: \n",
            "Train Loss 0.8289, Train Acc 74.94%, Test Loss 0.7706, Test Acc 76.25%\n",
            "Fold 2, Epoch 51: \n",
            "Train Loss 0.8278, Train Acc 74.97%, Test Loss 0.7692, Test Acc 76.26%\n",
            "Fold 2, Epoch 52: \n",
            "Train Loss 0.8268, Train Acc 74.98%, Test Loss 0.7679, Test Acc 76.28%\n",
            "Fold 2, Epoch 53: \n",
            "Train Loss 0.8254, Train Acc 75.01%, Test Loss 0.7668, Test Acc 76.31%\n",
            "Fold 2, Epoch 54: \n",
            "Train Loss 0.8233, Train Acc 75.05%, Test Loss 0.7656, Test Acc 76.33%\n",
            "Fold 2, Epoch 55: \n",
            "Train Loss 0.8223, Train Acc 75.07%, Test Loss 0.7645, Test Acc 76.36%\n",
            "Fold 2, Epoch 56: \n",
            "Train Loss 0.8210, Train Acc 75.10%, Test Loss 0.7636, Test Acc 76.37%\n",
            "Fold 2, Epoch 57: \n",
            "Train Loss 0.8206, Train Acc 75.10%, Test Loss 0.7627, Test Acc 76.39%\n",
            "Fold 2, Epoch 58: \n",
            "Train Loss 0.8200, Train Acc 75.11%, Test Loss 0.7621, Test Acc 76.39%\n",
            "Fold 2, Epoch 59: \n",
            "Train Loss 0.8187, Train Acc 75.14%, Test Loss 0.7610, Test Acc 76.41%\n",
            "Fold 2, Epoch 60: \n",
            "Train Loss 0.8173, Train Acc 75.17%, Test Loss 0.7602, Test Acc 76.44%\n",
            "Fold 2, Epoch 61: \n",
            "Train Loss 0.8167, Train Acc 75.18%, Test Loss 0.7594, Test Acc 76.45%\n",
            "Fold 2, Epoch 62: \n",
            "Train Loss 0.8154, Train Acc 75.20%, Test Loss 0.7588, Test Acc 76.46%\n",
            "Fold 2, Epoch 63: \n",
            "Train Loss 0.8148, Train Acc 75.22%, Test Loss 0.7582, Test Acc 76.47%\n",
            "Fold 2, Epoch 64: \n",
            "Train Loss 0.8137, Train Acc 75.25%, Test Loss 0.7575, Test Acc 76.49%\n",
            "Fold 2, Epoch 65: \n",
            "Train Loss 0.8129, Train Acc 75.26%, Test Loss 0.7568, Test Acc 76.50%\n",
            "Fold 2, Epoch 66: \n",
            "Train Loss 0.8118, Train Acc 75.28%, Test Loss 0.7563, Test Acc 76.51%\n",
            "Fold 2, Epoch 67: \n",
            "Train Loss 0.8113, Train Acc 75.30%, Test Loss 0.7557, Test Acc 76.52%\n",
            "Fold 2, Epoch 68: \n",
            "Train Loss 0.8106, Train Acc 75.31%, Test Loss 0.7552, Test Acc 76.53%\n",
            "Fold 2, Epoch 69: \n",
            "Train Loss 0.8102, Train Acc 75.32%, Test Loss 0.7546, Test Acc 76.55%\n",
            "Fold 2, Epoch 70: \n",
            "Train Loss 0.8093, Train Acc 75.34%, Test Loss 0.7543, Test Acc 76.56%\n",
            "Fold 2, Epoch 71: \n",
            "Train Loss 0.8089, Train Acc 75.35%, Test Loss 0.7538, Test Acc 76.57%\n",
            "Fold 2, Epoch 72: \n",
            "Train Loss 0.8081, Train Acc 75.37%, Test Loss 0.7532, Test Acc 76.58%\n",
            "Fold 2, Epoch 73: \n",
            "Train Loss 0.8077, Train Acc 75.38%, Test Loss 0.7529, Test Acc 76.58%\n",
            "Fold 2, Epoch 74: \n",
            "Train Loss 0.8071, Train Acc 75.39%, Test Loss 0.7524, Test Acc 76.60%\n",
            "Fold 2, Epoch 75: \n",
            "Train Loss 0.8061, Train Acc 75.42%, Test Loss 0.7520, Test Acc 76.60%\n",
            "Fold 2, Epoch 76: \n",
            "Train Loss 0.8058, Train Acc 75.43%, Test Loss 0.7516, Test Acc 76.62%\n",
            "Fold 2, Epoch 77: \n",
            "Train Loss 0.8052, Train Acc 75.44%, Test Loss 0.7510, Test Acc 76.63%\n",
            "Fold 2, Epoch 78: \n",
            "Train Loss 0.8050, Train Acc 75.44%, Test Loss 0.7507, Test Acc 76.64%\n",
            "Fold 2, Epoch 79: \n",
            "Train Loss 0.8044, Train Acc 75.46%, Test Loss 0.7502, Test Acc 76.65%\n",
            "Fold 2, Epoch 80: \n",
            "Train Loss 0.8042, Train Acc 75.46%, Test Loss 0.7499, Test Acc 76.65%\n",
            "Fold 2, Epoch 81: \n",
            "Train Loss 0.8037, Train Acc 75.47%, Test Loss 0.7496, Test Acc 76.67%\n",
            "Fold 2, Epoch 82: \n",
            "Train Loss 0.8032, Train Acc 75.49%, Test Loss 0.7490, Test Acc 76.68%\n",
            "Fold 2, Epoch 83: \n",
            "Train Loss 0.8028, Train Acc 75.49%, Test Loss 0.7489, Test Acc 76.68%\n",
            "Fold 2, Epoch 84: \n",
            "Train Loss 0.8020, Train Acc 75.52%, Test Loss 0.7485, Test Acc 76.69%\n",
            "Fold 2, Epoch 85: \n",
            "Train Loss 0.8020, Train Acc 75.52%, Test Loss 0.7482, Test Acc 76.70%\n",
            "Fold 2, Epoch 86: \n",
            "Train Loss 0.8010, Train Acc 75.54%, Test Loss 0.7479, Test Acc 76.71%\n",
            "Fold 2, Epoch 87: \n",
            "Train Loss 0.8014, Train Acc 75.53%, Test Loss 0.7475, Test Acc 76.71%\n",
            "Fold 2, Epoch 88: \n",
            "Train Loss 0.8011, Train Acc 75.54%, Test Loss 0.7472, Test Acc 76.72%\n",
            "Fold 2, Epoch 89: \n",
            "Train Loss 0.8004, Train Acc 75.56%, Test Loss 0.7468, Test Acc 76.74%\n",
            "Fold 2, Epoch 90: \n",
            "Train Loss 0.7996, Train Acc 75.57%, Test Loss 0.7464, Test Acc 76.74%\n",
            "Fold 2, Epoch 91: \n",
            "Train Loss 0.8000, Train Acc 75.57%, Test Loss 0.7463, Test Acc 76.75%\n",
            "Fold 2, Epoch 92: \n",
            "Train Loss 0.7994, Train Acc 75.58%, Test Loss 0.7459, Test Acc 76.76%\n",
            "Fold 2, Epoch 93: \n",
            "Train Loss 0.7989, Train Acc 75.60%, Test Loss 0.7454, Test Acc 76.78%\n",
            "Fold 2, Epoch 94: \n",
            "Train Loss 0.7991, Train Acc 75.59%, Test Loss 0.7455, Test Acc 76.77%\n",
            "Fold 2, Epoch 95: \n",
            "Train Loss 0.7983, Train Acc 75.61%, Test Loss 0.7451, Test Acc 76.78%\n",
            "Fold 2, Epoch 96: \n",
            "Train Loss 0.7979, Train Acc 75.62%, Test Loss 0.7450, Test Acc 76.79%\n",
            "Fold 2, Epoch 97: \n",
            "Train Loss 0.7974, Train Acc 75.64%, Test Loss 0.7444, Test Acc 76.80%\n",
            "Fold 2, Epoch 98: \n",
            "Train Loss 0.7972, Train Acc 75.64%, Test Loss 0.7443, Test Acc 76.80%\n",
            "Fold 2, Epoch 99: \n",
            "Train Loss 0.7968, Train Acc 75.65%, Test Loss 0.7439, Test Acc 76.81%\n",
            "Fold 2, Epoch 100: \n",
            "Train Loss 0.7966, Train Acc 75.65%, Test Loss 0.7436, Test Acc 76.82%\n",
            "\n",
            "=== Fold 3/5 ===\n",
            "Fold 3, Epoch 1: \n",
            "Train Loss 1.7829, Train Acc 56.24%, Test Loss 1.3063, Test Acc 61.67%\n",
            "Fold 3, Epoch 2: \n",
            "Train Loss 1.2517, Train Acc 63.13%, Test Loss 1.1716, Test Acc 65.10%\n",
            "Fold 3, Epoch 3: \n",
            "Train Loss 1.1706, Train Acc 65.15%, Test Loss 1.1203, Test Acc 66.46%\n",
            "Fold 3, Epoch 4: \n",
            "Train Loss 1.1298, Train Acc 66.32%, Test Loss 1.0841, Test Acc 67.65%\n",
            "Fold 3, Epoch 5: \n",
            "Train Loss 1.0992, Train Acc 67.27%, Test Loss 1.0529, Test Acc 68.54%\n",
            "Fold 3, Epoch 6: \n",
            "Train Loss 1.0746, Train Acc 68.02%, Test Loss 1.0267, Test Acc 69.36%\n",
            "Fold 3, Epoch 7: \n",
            "Train Loss 1.0508, Train Acc 68.77%, Test Loss 0.9992, Test Acc 70.23%\n",
            "Fold 3, Epoch 8: \n",
            "Train Loss 1.0277, Train Acc 69.48%, Test Loss 0.9724, Test Acc 71.02%\n",
            "Fold 3, Epoch 9: \n",
            "Train Loss 1.0066, Train Acc 70.11%, Test Loss 0.9516, Test Acc 71.60%\n",
            "Fold 3, Epoch 10: \n",
            "Train Loss 0.9898, Train Acc 70.61%, Test Loss 0.9343, Test Acc 72.11%\n",
            "Fold 3, Epoch 11: \n",
            "Train Loss 0.9746, Train Acc 71.06%, Test Loss 0.9198, Test Acc 72.53%\n",
            "Fold 3, Epoch 12: \n",
            "Train Loss 0.9627, Train Acc 71.41%, Test Loss 0.9081, Test Acc 72.88%\n",
            "Fold 3, Epoch 13: \n",
            "Train Loss 0.9526, Train Acc 71.72%, Test Loss 0.8981, Test Acc 73.18%\n",
            "Fold 3, Epoch 14: \n",
            "Train Loss 0.9440, Train Acc 71.97%, Test Loss 0.8888, Test Acc 73.42%\n",
            "Fold 3, Epoch 15: \n",
            "Train Loss 0.9362, Train Acc 72.22%, Test Loss 0.8810, Test Acc 73.65%\n",
            "Fold 3, Epoch 16: \n",
            "Train Loss 0.9294, Train Acc 72.42%, Test Loss 0.8732, Test Acc 73.88%\n",
            "Fold 3, Epoch 17: \n",
            "Train Loss 0.9226, Train Acc 72.63%, Test Loss 0.8660, Test Acc 74.07%\n",
            "Fold 3, Epoch 18: \n",
            "Train Loss 0.9161, Train Acc 72.81%, Test Loss 0.8601, Test Acc 74.25%\n",
            "Fold 3, Epoch 19: \n",
            "Train Loss 0.9114, Train Acc 72.95%, Test Loss 0.8539, Test Acc 74.41%\n",
            "Fold 3, Epoch 20: \n",
            "Train Loss 0.9057, Train Acc 73.10%, Test Loss 0.8486, Test Acc 74.54%\n",
            "Fold 3, Epoch 21: \n",
            "Train Loss 0.9007, Train Acc 73.24%, Test Loss 0.8435, Test Acc 74.68%\n",
            "Fold 3, Epoch 22: \n",
            "Train Loss 0.8964, Train Acc 73.35%, Test Loss 0.8390, Test Acc 74.78%\n",
            "Fold 3, Epoch 23: \n",
            "Train Loss 0.8924, Train Acc 73.45%, Test Loss 0.8344, Test Acc 74.90%\n",
            "Fold 3, Epoch 24: \n",
            "Train Loss 0.8886, Train Acc 73.55%, Test Loss 0.8308, Test Acc 74.98%\n",
            "Fold 3, Epoch 25: \n",
            "Train Loss 0.8845, Train Acc 73.66%, Test Loss 0.8268, Test Acc 75.06%\n",
            "Fold 3, Epoch 26: \n",
            "Train Loss 0.8820, Train Acc 73.72%, Test Loss 0.8234, Test Acc 75.15%\n",
            "Fold 3, Epoch 27: \n",
            "Train Loss 0.8783, Train Acc 73.81%, Test Loss 0.8201, Test Acc 75.23%\n",
            "Fold 3, Epoch 28: \n",
            "Train Loss 0.8745, Train Acc 73.91%, Test Loss 0.8171, Test Acc 75.29%\n",
            "Fold 3, Epoch 29: \n",
            "Train Loss 0.8722, Train Acc 73.97%, Test Loss 0.8137, Test Acc 75.37%\n",
            "Fold 3, Epoch 30: \n",
            "Train Loss 0.8691, Train Acc 74.04%, Test Loss 0.8115, Test Acc 75.41%\n",
            "Fold 3, Epoch 31: \n",
            "Train Loss 0.8669, Train Acc 74.09%, Test Loss 0.8086, Test Acc 75.49%\n",
            "Fold 3, Epoch 32: \n",
            "Train Loss 0.8638, Train Acc 74.17%, Test Loss 0.8062, Test Acc 75.54%\n",
            "Fold 3, Epoch 33: \n",
            "Train Loss 0.8620, Train Acc 74.21%, Test Loss 0.8043, Test Acc 75.58%\n",
            "Fold 3, Epoch 34: \n",
            "Train Loss 0.8599, Train Acc 74.27%, Test Loss 0.8019, Test Acc 75.64%\n",
            "Fold 3, Epoch 35: \n",
            "Train Loss 0.8573, Train Acc 74.33%, Test Loss 0.8002, Test Acc 75.69%\n",
            "Fold 3, Epoch 36: \n",
            "Train Loss 0.8554, Train Acc 74.39%, Test Loss 0.7985, Test Acc 75.72%\n",
            "Fold 3, Epoch 37: \n",
            "Train Loss 0.8537, Train Acc 74.43%, Test Loss 0.7962, Test Acc 75.78%\n",
            "Fold 3, Epoch 38: \n",
            "Train Loss 0.8516, Train Acc 74.47%, Test Loss 0.7944, Test Acc 75.82%\n",
            "Fold 3, Epoch 39: \n",
            "Train Loss 0.8502, Train Acc 74.51%, Test Loss 0.7930, Test Acc 75.84%\n",
            "Fold 3, Epoch 40: \n",
            "Train Loss 0.8481, Train Acc 74.56%, Test Loss 0.7912, Test Acc 75.89%\n",
            "Fold 3, Epoch 41: \n",
            "Train Loss 0.8463, Train Acc 74.60%, Test Loss 0.7893, Test Acc 75.93%\n",
            "Fold 3, Epoch 42: \n",
            "Train Loss 0.8445, Train Acc 74.65%, Test Loss 0.7874, Test Acc 75.97%\n",
            "Fold 3, Epoch 43: \n",
            "Train Loss 0.8429, Train Acc 74.68%, Test Loss 0.7859, Test Acc 76.01%\n",
            "Fold 3, Epoch 44: \n",
            "Train Loss 0.8419, Train Acc 74.70%, Test Loss 0.7845, Test Acc 76.03%\n",
            "Fold 3, Epoch 45: \n",
            "Train Loss 0.8399, Train Acc 74.75%, Test Loss 0.7828, Test Acc 76.07%\n",
            "Fold 3, Epoch 46: \n",
            "Train Loss 0.8384, Train Acc 74.79%, Test Loss 0.7813, Test Acc 76.10%\n",
            "Fold 3, Epoch 47: \n",
            "Train Loss 0.8373, Train Acc 74.81%, Test Loss 0.7802, Test Acc 76.13%\n",
            "Fold 3, Epoch 48: \n",
            "Train Loss 0.8368, Train Acc 74.81%, Test Loss 0.7785, Test Acc 76.16%\n",
            "Fold 3, Epoch 49: \n",
            "Train Loss 0.8346, Train Acc 74.87%, Test Loss 0.7773, Test Acc 76.19%\n",
            "Fold 3, Epoch 50: \n",
            "Train Loss 0.8331, Train Acc 74.90%, Test Loss 0.7759, Test Acc 76.22%\n",
            "Fold 3, Epoch 51: \n",
            "Train Loss 0.8321, Train Acc 74.92%, Test Loss 0.7748, Test Acc 76.23%\n",
            "Fold 3, Epoch 52: \n",
            "Train Loss 0.8312, Train Acc 74.94%, Test Loss 0.7734, Test Acc 76.26%\n",
            "Fold 3, Epoch 53: \n",
            "Train Loss 0.8293, Train Acc 74.97%, Test Loss 0.7720, Test Acc 76.30%\n",
            "Fold 3, Epoch 54: \n",
            "Train Loss 0.8278, Train Acc 75.01%, Test Loss 0.7708, Test Acc 76.31%\n",
            "Fold 3, Epoch 55: \n",
            "Train Loss 0.8263, Train Acc 75.04%, Test Loss 0.7696, Test Acc 76.33%\n",
            "Fold 3, Epoch 56: \n",
            "Train Loss 0.8254, Train Acc 75.05%, Test Loss 0.7680, Test Acc 76.36%\n",
            "Fold 3, Epoch 57: \n",
            "Train Loss 0.8245, Train Acc 75.08%, Test Loss 0.7667, Test Acc 76.39%\n",
            "Fold 3, Epoch 58: \n",
            "Train Loss 0.8237, Train Acc 75.09%, Test Loss 0.7656, Test Acc 76.40%\n",
            "Fold 3, Epoch 59: \n",
            "Train Loss 0.8228, Train Acc 75.10%, Test Loss 0.7643, Test Acc 76.42%\n",
            "Fold 3, Epoch 60: \n",
            "Train Loss 0.8207, Train Acc 75.15%, Test Loss 0.7630, Test Acc 76.45%\n",
            "Fold 3, Epoch 61: \n",
            "Train Loss 0.8203, Train Acc 75.15%, Test Loss 0.7622, Test Acc 76.46%\n",
            "Fold 3, Epoch 62: \n",
            "Train Loss 0.8188, Train Acc 75.17%, Test Loss 0.7612, Test Acc 76.48%\n",
            "Fold 3, Epoch 63: \n",
            "Train Loss 0.8178, Train Acc 75.20%, Test Loss 0.7603, Test Acc 76.50%\n",
            "Fold 3, Epoch 64: \n",
            "Train Loss 0.8164, Train Acc 75.23%, Test Loss 0.7593, Test Acc 76.52%\n",
            "Fold 3, Epoch 65: \n",
            "Train Loss 0.8159, Train Acc 75.23%, Test Loss 0.7586, Test Acc 76.53%\n",
            "Fold 3, Epoch 66: \n",
            "Train Loss 0.8154, Train Acc 75.24%, Test Loss 0.7576, Test Acc 76.55%\n",
            "Fold 3, Epoch 67: \n",
            "Train Loss 0.8140, Train Acc 75.27%, Test Loss 0.7567, Test Acc 76.57%\n",
            "Fold 3, Epoch 68: \n",
            "Train Loss 0.8131, Train Acc 75.29%, Test Loss 0.7561, Test Acc 76.58%\n",
            "Fold 3, Epoch 69: \n",
            "Train Loss 0.8130, Train Acc 75.29%, Test Loss 0.7551, Test Acc 76.60%\n",
            "Fold 3, Epoch 70: \n",
            "Train Loss 0.8118, Train Acc 75.32%, Test Loss 0.7547, Test Acc 76.60%\n",
            "Fold 3, Epoch 71: \n",
            "Train Loss 0.8115, Train Acc 75.32%, Test Loss 0.7541, Test Acc 76.61%\n",
            "Fold 3, Epoch 72: \n",
            "Train Loss 0.8105, Train Acc 75.35%, Test Loss 0.7536, Test Acc 76.63%\n",
            "Fold 3, Epoch 73: \n",
            "Train Loss 0.8096, Train Acc 75.36%, Test Loss 0.7530, Test Acc 76.64%\n",
            "Fold 3, Epoch 74: \n",
            "Train Loss 0.8093, Train Acc 75.37%, Test Loss 0.7523, Test Acc 76.65%\n",
            "Fold 3, Epoch 75: \n",
            "Train Loss 0.8082, Train Acc 75.39%, Test Loss 0.7518, Test Acc 76.67%\n",
            "Fold 3, Epoch 76: \n",
            "Train Loss 0.8080, Train Acc 75.39%, Test Loss 0.7512, Test Acc 76.67%\n",
            "Fold 3, Epoch 77: \n",
            "Train Loss 0.8074, Train Acc 75.41%, Test Loss 0.7508, Test Acc 76.68%\n",
            "Fold 3, Epoch 78: \n",
            "Train Loss 0.8065, Train Acc 75.43%, Test Loss 0.7501, Test Acc 76.70%\n",
            "Fold 3, Epoch 79: \n",
            "Train Loss 0.8059, Train Acc 75.44%, Test Loss 0.7499, Test Acc 76.70%\n",
            "Fold 3, Epoch 80: \n",
            "Train Loss 0.8054, Train Acc 75.45%, Test Loss 0.7494, Test Acc 76.72%\n",
            "Fold 3, Epoch 81: \n",
            "Train Loss 0.8051, Train Acc 75.45%, Test Loss 0.7489, Test Acc 76.73%\n",
            "Fold 3, Epoch 82: \n",
            "Train Loss 0.8049, Train Acc 75.46%, Test Loss 0.7484, Test Acc 76.74%\n",
            "Fold 3, Epoch 83: \n",
            "Train Loss 0.8036, Train Acc 75.50%, Test Loss 0.7480, Test Acc 76.75%\n",
            "Fold 3, Epoch 84: \n",
            "Train Loss 0.8039, Train Acc 75.48%, Test Loss 0.7476, Test Acc 76.75%\n",
            "Fold 3, Epoch 85: \n",
            "Train Loss 0.8032, Train Acc 75.50%, Test Loss 0.7471, Test Acc 76.77%\n",
            "Fold 3, Epoch 86: \n",
            "Train Loss 0.8024, Train Acc 75.52%, Test Loss 0.7468, Test Acc 76.77%\n",
            "Fold 3, Epoch 87: \n",
            "Train Loss 0.8020, Train Acc 75.53%, Test Loss 0.7464, Test Acc 76.79%\n",
            "Fold 3, Epoch 88: \n",
            "Train Loss 0.8014, Train Acc 75.54%, Test Loss 0.7460, Test Acc 76.80%\n",
            "Fold 3, Epoch 89: \n",
            "Train Loss 0.8015, Train Acc 75.55%, Test Loss 0.7458, Test Acc 76.80%\n",
            "Fold 3, Epoch 90: \n",
            "Train Loss 0.8009, Train Acc 75.56%, Test Loss 0.7452, Test Acc 76.82%\n",
            "Fold 3, Epoch 91: \n",
            "Train Loss 0.8009, Train Acc 75.56%, Test Loss 0.7452, Test Acc 76.81%\n",
            "Fold 3, Epoch 92: \n",
            "Train Loss 0.7999, Train Acc 75.58%, Test Loss 0.7446, Test Acc 76.83%\n",
            "Fold 3, Epoch 93: \n",
            "Train Loss 0.7997, Train Acc 75.59%, Test Loss 0.7447, Test Acc 76.83%\n",
            "Fold 3, Epoch 94: \n",
            "Train Loss 0.7991, Train Acc 75.61%, Test Loss 0.7442, Test Acc 76.84%\n",
            "Fold 3, Epoch 95: \n",
            "Train Loss 0.7992, Train Acc 75.60%, Test Loss 0.7438, Test Acc 76.85%\n",
            "Fold 3, Epoch 96: \n",
            "Train Loss 0.7990, Train Acc 75.60%, Test Loss 0.7435, Test Acc 76.85%\n",
            "Fold 3, Epoch 97: \n",
            "Train Loss 0.7983, Train Acc 75.62%, Test Loss 0.7431, Test Acc 76.86%\n",
            "Fold 3, Epoch 98: \n",
            "Train Loss 0.7981, Train Acc 75.63%, Test Loss 0.7428, Test Acc 76.87%\n",
            "Fold 3, Epoch 99: \n",
            "Train Loss 0.7975, Train Acc 75.64%, Test Loss 0.7427, Test Acc 76.88%\n",
            "Fold 3, Epoch 100: \n",
            "Train Loss 0.7977, Train Acc 75.63%, Test Loss 0.7423, Test Acc 76.88%\n",
            "\n",
            "=== Fold 4/5 ===\n",
            "Fold 4, Epoch 1: \n",
            "Train Loss 1.7688, Train Acc 56.69%, Test Loss 1.3074, Test Acc 61.74%\n",
            "Fold 4, Epoch 2: \n",
            "Train Loss 1.2562, Train Acc 62.94%, Test Loss 1.1803, Test Acc 64.87%\n",
            "Fold 4, Epoch 3: \n",
            "Train Loss 1.1760, Train Acc 64.99%, Test Loss 1.1189, Test Acc 66.50%\n",
            "Fold 4, Epoch 4: \n",
            "Train Loss 1.1300, Train Acc 66.33%, Test Loss 1.0787, Test Acc 67.79%\n",
            "Fold 4, Epoch 5: \n",
            "Train Loss 1.0981, Train Acc 67.27%, Test Loss 1.0514, Test Acc 68.51%\n",
            "Fold 4, Epoch 6: \n",
            "Train Loss 1.0742, Train Acc 67.98%, Test Loss 1.0260, Test Acc 69.26%\n",
            "Fold 4, Epoch 7: \n",
            "Train Loss 1.0516, Train Acc 68.68%, Test Loss 0.9968, Test Acc 70.18%\n",
            "Fold 4, Epoch 8: \n",
            "Train Loss 1.0264, Train Acc 69.45%, Test Loss 0.9717, Test Acc 70.94%\n",
            "Fold 4, Epoch 9: \n",
            "Train Loss 1.0069, Train Acc 70.07%, Test Loss 0.9530, Test Acc 71.56%\n",
            "Fold 4, Epoch 10: \n",
            "Train Loss 0.9910, Train Acc 70.58%, Test Loss 0.9364, Test Acc 72.06%\n",
            "Fold 4, Epoch 11: \n",
            "Train Loss 0.9773, Train Acc 71.01%, Test Loss 0.9227, Test Acc 72.49%\n",
            "Fold 4, Epoch 12: \n",
            "Train Loss 0.9653, Train Acc 71.39%, Test Loss 0.9107, Test Acc 72.86%\n",
            "Fold 4, Epoch 13: \n",
            "Train Loss 0.9552, Train Acc 71.71%, Test Loss 0.9007, Test Acc 73.17%\n",
            "Fold 4, Epoch 14: \n",
            "Train Loss 0.9462, Train Acc 71.98%, Test Loss 0.8922, Test Acc 73.43%\n",
            "Fold 4, Epoch 15: \n",
            "Train Loss 0.9386, Train Acc 72.22%, Test Loss 0.8844, Test Acc 73.66%\n",
            "Fold 4, Epoch 16: \n",
            "Train Loss 0.9319, Train Acc 72.43%, Test Loss 0.8769, Test Acc 73.87%\n",
            "Fold 4, Epoch 17: \n",
            "Train Loss 0.9256, Train Acc 72.62%, Test Loss 0.8701, Test Acc 74.06%\n",
            "Fold 4, Epoch 18: \n",
            "Train Loss 0.9191, Train Acc 72.81%, Test Loss 0.8632, Test Acc 74.27%\n",
            "Fold 4, Epoch 19: \n",
            "Train Loss 0.9137, Train Acc 72.97%, Test Loss 0.8567, Test Acc 74.45%\n",
            "Fold 4, Epoch 20: \n",
            "Train Loss 0.9082, Train Acc 73.14%, Test Loss 0.8509, Test Acc 74.59%\n",
            "Fold 4, Epoch 21: \n",
            "Train Loss 0.9032, Train Acc 73.28%, Test Loss 0.8459, Test Acc 74.72%\n",
            "Fold 4, Epoch 22: \n",
            "Train Loss 0.8989, Train Acc 73.40%, Test Loss 0.8414, Test Acc 74.84%\n",
            "Fold 4, Epoch 23: \n",
            "Train Loss 0.8948, Train Acc 73.51%, Test Loss 0.8367, Test Acc 74.95%\n",
            "Fold 4, Epoch 24: \n",
            "Train Loss 0.8909, Train Acc 73.62%, Test Loss 0.8328, Test Acc 75.05%\n",
            "Fold 4, Epoch 25: \n",
            "Train Loss 0.8869, Train Acc 73.73%, Test Loss 0.8291, Test Acc 75.13%\n",
            "Fold 4, Epoch 26: \n",
            "Train Loss 0.8836, Train Acc 73.82%, Test Loss 0.8253, Test Acc 75.23%\n",
            "Fold 4, Epoch 27: \n",
            "Train Loss 0.8804, Train Acc 73.89%, Test Loss 0.8221, Test Acc 75.31%\n",
            "Fold 4, Epoch 28: \n",
            "Train Loss 0.8772, Train Acc 73.97%, Test Loss 0.8181, Test Acc 75.38%\n",
            "Fold 4, Epoch 29: \n",
            "Train Loss 0.8735, Train Acc 74.06%, Test Loss 0.8148, Test Acc 75.45%\n",
            "Fold 4, Epoch 30: \n",
            "Train Loss 0.8708, Train Acc 74.11%, Test Loss 0.8114, Test Acc 75.51%\n",
            "Fold 4, Epoch 31: \n",
            "Train Loss 0.8680, Train Acc 74.18%, Test Loss 0.8086, Test Acc 75.57%\n",
            "Fold 4, Epoch 32: \n",
            "Train Loss 0.8653, Train Acc 74.24%, Test Loss 0.8059, Test Acc 75.63%\n",
            "Fold 4, Epoch 33: \n",
            "Train Loss 0.8631, Train Acc 74.29%, Test Loss 0.8030, Test Acc 75.68%\n",
            "Fold 4, Epoch 34: \n",
            "Train Loss 0.8596, Train Acc 74.37%, Test Loss 0.8008, Test Acc 75.74%\n",
            "Fold 4, Epoch 35: \n",
            "Train Loss 0.8574, Train Acc 74.42%, Test Loss 0.7983, Test Acc 75.79%\n",
            "Fold 4, Epoch 36: \n",
            "Train Loss 0.8552, Train Acc 74.46%, Test Loss 0.7960, Test Acc 75.83%\n",
            "Fold 4, Epoch 37: \n",
            "Train Loss 0.8534, Train Acc 74.51%, Test Loss 0.7941, Test Acc 75.87%\n",
            "Fold 4, Epoch 38: \n",
            "Train Loss 0.8510, Train Acc 74.55%, Test Loss 0.7917, Test Acc 75.91%\n",
            "Fold 4, Epoch 39: \n",
            "Train Loss 0.8494, Train Acc 74.58%, Test Loss 0.7899, Test Acc 75.95%\n",
            "Fold 4, Epoch 40: \n",
            "Train Loss 0.8472, Train Acc 74.64%, Test Loss 0.7878, Test Acc 75.98%\n",
            "Fold 4, Epoch 41: \n",
            "Train Loss 0.8455, Train Acc 74.67%, Test Loss 0.7860, Test Acc 76.03%\n",
            "Fold 4, Epoch 42: \n",
            "Train Loss 0.8437, Train Acc 74.71%, Test Loss 0.7842, Test Acc 76.06%\n",
            "Fold 4, Epoch 43: \n",
            "Train Loss 0.8415, Train Acc 74.75%, Test Loss 0.7824, Test Acc 76.09%\n",
            "Fold 4, Epoch 44: \n",
            "Train Loss 0.8399, Train Acc 74.79%, Test Loss 0.7807, Test Acc 76.13%\n",
            "Fold 4, Epoch 45: \n",
            "Train Loss 0.8385, Train Acc 74.82%, Test Loss 0.7792, Test Acc 76.15%\n",
            "Fold 4, Epoch 46: \n",
            "Train Loss 0.8366, Train Acc 74.85%, Test Loss 0.7773, Test Acc 76.19%\n",
            "Fold 4, Epoch 47: \n",
            "Train Loss 0.8355, Train Acc 74.87%, Test Loss 0.7758, Test Acc 76.22%\n",
            "Fold 4, Epoch 48: \n",
            "Train Loss 0.8339, Train Acc 74.91%, Test Loss 0.7740, Test Acc 76.25%\n",
            "Fold 4, Epoch 49: \n",
            "Train Loss 0.8325, Train Acc 74.93%, Test Loss 0.7725, Test Acc 76.27%\n",
            "Fold 4, Epoch 50: \n",
            "Train Loss 0.8307, Train Acc 74.96%, Test Loss 0.7709, Test Acc 76.30%\n",
            "Fold 4, Epoch 51: \n",
            "Train Loss 0.8294, Train Acc 74.98%, Test Loss 0.7695, Test Acc 76.32%\n",
            "Fold 4, Epoch 52: \n",
            "Train Loss 0.8279, Train Acc 75.01%, Test Loss 0.7679, Test Acc 76.35%\n",
            "Fold 4, Epoch 53: \n",
            "Train Loss 0.8271, Train Acc 75.02%, Test Loss 0.7669, Test Acc 76.37%\n",
            "Fold 4, Epoch 54: \n",
            "Train Loss 0.8254, Train Acc 75.06%, Test Loss 0.7654, Test Acc 76.40%\n",
            "Fold 4, Epoch 55: \n",
            "Train Loss 0.8245, Train Acc 75.07%, Test Loss 0.7642, Test Acc 76.42%\n",
            "Fold 4, Epoch 56: \n",
            "Train Loss 0.8225, Train Acc 75.12%, Test Loss 0.7634, Test Acc 76.42%\n",
            "Fold 4, Epoch 57: \n",
            "Train Loss 0.8214, Train Acc 75.14%, Test Loss 0.7622, Test Acc 76.45%\n",
            "Fold 4, Epoch 58: \n",
            "Train Loss 0.8205, Train Acc 75.16%, Test Loss 0.7613, Test Acc 76.48%\n",
            "Fold 4, Epoch 59: \n",
            "Train Loss 0.8192, Train Acc 75.18%, Test Loss 0.7603, Test Acc 76.50%\n",
            "Fold 4, Epoch 60: \n",
            "Train Loss 0.8186, Train Acc 75.19%, Test Loss 0.7597, Test Acc 76.51%\n",
            "Fold 4, Epoch 61: \n",
            "Train Loss 0.8170, Train Acc 75.22%, Test Loss 0.7589, Test Acc 76.52%\n",
            "Fold 4, Epoch 62: \n",
            "Train Loss 0.8166, Train Acc 75.24%, Test Loss 0.7582, Test Acc 76.54%\n",
            "Fold 4, Epoch 63: \n",
            "Train Loss 0.8151, Train Acc 75.26%, Test Loss 0.7574, Test Acc 76.56%\n",
            "Fold 4, Epoch 64: \n",
            "Train Loss 0.8147, Train Acc 75.26%, Test Loss 0.7568, Test Acc 76.57%\n",
            "Fold 4, Epoch 65: \n",
            "Train Loss 0.8136, Train Acc 75.29%, Test Loss 0.7563, Test Acc 76.58%\n",
            "Fold 4, Epoch 66: \n",
            "Train Loss 0.8131, Train Acc 75.31%, Test Loss 0.7557, Test Acc 76.59%\n",
            "Fold 4, Epoch 67: \n",
            "Train Loss 0.8125, Train Acc 75.32%, Test Loss 0.7552, Test Acc 76.61%\n",
            "Fold 4, Epoch 68: \n",
            "Train Loss 0.8113, Train Acc 75.35%, Test Loss 0.7543, Test Acc 76.62%\n",
            "Fold 4, Epoch 69: \n",
            "Train Loss 0.8106, Train Acc 75.36%, Test Loss 0.7539, Test Acc 76.63%\n",
            "Fold 4, Epoch 70: \n",
            "Train Loss 0.8099, Train Acc 75.38%, Test Loss 0.7533, Test Acc 76.66%\n",
            "Fold 4, Epoch 71: \n",
            "Train Loss 0.8101, Train Acc 75.37%, Test Loss 0.7526, Test Acc 76.67%\n",
            "Fold 4, Epoch 72: \n",
            "Train Loss 0.8088, Train Acc 75.40%, Test Loss 0.7522, Test Acc 76.68%\n",
            "Fold 4, Epoch 73: \n",
            "Train Loss 0.8081, Train Acc 75.42%, Test Loss 0.7517, Test Acc 76.70%\n",
            "Fold 4, Epoch 74: \n",
            "Train Loss 0.8077, Train Acc 75.42%, Test Loss 0.7512, Test Acc 76.70%\n",
            "Fold 4, Epoch 75: \n",
            "Train Loss 0.8064, Train Acc 75.46%, Test Loss 0.7504, Test Acc 76.72%\n",
            "Fold 4, Epoch 76: \n",
            "Train Loss 0.8057, Train Acc 75.48%, Test Loss 0.7500, Test Acc 76.73%\n",
            "Fold 4, Epoch 77: \n",
            "Train Loss 0.8058, Train Acc 75.46%, Test Loss 0.7499, Test Acc 76.74%\n",
            "Fold 4, Epoch 78: \n",
            "Train Loss 0.8051, Train Acc 75.49%, Test Loss 0.7492, Test Acc 76.75%\n",
            "Fold 4, Epoch 79: \n",
            "Train Loss 0.8047, Train Acc 75.49%, Test Loss 0.7487, Test Acc 76.77%\n",
            "Fold 4, Epoch 80: \n",
            "Train Loss 0.8036, Train Acc 75.52%, Test Loss 0.7483, Test Acc 76.77%\n",
            "Fold 4, Epoch 81: \n",
            "Train Loss 0.8032, Train Acc 75.53%, Test Loss 0.7478, Test Acc 76.78%\n",
            "Fold 4, Epoch 82: \n",
            "Train Loss 0.8025, Train Acc 75.55%, Test Loss 0.7476, Test Acc 76.78%\n",
            "Fold 4, Epoch 83: \n",
            "Train Loss 0.8025, Train Acc 75.54%, Test Loss 0.7468, Test Acc 76.80%\n",
            "Fold 4, Epoch 84: \n",
            "Train Loss 0.8025, Train Acc 75.54%, Test Loss 0.7467, Test Acc 76.80%\n",
            "Fold 4, Epoch 85: \n",
            "Train Loss 0.8014, Train Acc 75.57%, Test Loss 0.7462, Test Acc 76.81%\n",
            "Fold 4, Epoch 86: \n",
            "Train Loss 0.8011, Train Acc 75.58%, Test Loss 0.7459, Test Acc 76.82%\n",
            "Fold 4, Epoch 87: \n",
            "Train Loss 0.8002, Train Acc 75.60%, Test Loss 0.7453, Test Acc 76.83%\n",
            "Fold 4, Epoch 88: \n",
            "Train Loss 0.8003, Train Acc 75.60%, Test Loss 0.7449, Test Acc 76.84%\n",
            "Fold 4, Epoch 89: \n",
            "Train Loss 0.7996, Train Acc 75.62%, Test Loss 0.7447, Test Acc 76.85%\n",
            "Fold 4, Epoch 90: \n",
            "Train Loss 0.7991, Train Acc 75.63%, Test Loss 0.7443, Test Acc 76.86%\n",
            "Fold 4, Epoch 91: \n",
            "Train Loss 0.7987, Train Acc 75.64%, Test Loss 0.7438, Test Acc 76.88%\n",
            "Fold 4, Epoch 92: \n",
            "Train Loss 0.7987, Train Acc 75.63%, Test Loss 0.7438, Test Acc 76.87%\n",
            "Fold 4, Epoch 93: \n",
            "Train Loss 0.7975, Train Acc 75.66%, Test Loss 0.7434, Test Acc 76.88%\n",
            "Fold 4, Epoch 94: \n",
            "Train Loss 0.7980, Train Acc 75.65%, Test Loss 0.7429, Test Acc 76.88%\n",
            "Fold 4, Epoch 95: \n",
            "Train Loss 0.7974, Train Acc 75.67%, Test Loss 0.7425, Test Acc 76.90%\n",
            "Fold 4, Epoch 96: \n",
            "Train Loss 0.7974, Train Acc 75.66%, Test Loss 0.7423, Test Acc 76.91%\n",
            "Fold 4, Epoch 97: \n",
            "Train Loss 0.7965, Train Acc 75.69%, Test Loss 0.7421, Test Acc 76.91%\n",
            "Fold 4, Epoch 98: \n",
            "Train Loss 0.7962, Train Acc 75.69%, Test Loss 0.7415, Test Acc 76.92%\n",
            "Fold 4, Epoch 99: \n",
            "Train Loss 0.7959, Train Acc 75.70%, Test Loss 0.7412, Test Acc 76.93%\n",
            "Fold 4, Epoch 100: \n",
            "Train Loss 0.7955, Train Acc 75.71%, Test Loss 0.7411, Test Acc 76.92%\n",
            "\n",
            "=== Fold 5/5 ===\n",
            "Fold 5, Epoch 1: \n",
            "Train Loss 1.7875, Train Acc 56.41%, Test Loss 1.3151, Test Acc 61.66%\n",
            "Fold 5, Epoch 2: \n",
            "Train Loss 1.2591, Train Acc 62.90%, Test Loss 1.1882, Test Acc 64.64%\n",
            "Fold 5, Epoch 3: \n",
            "Train Loss 1.1819, Train Acc 64.78%, Test Loss 1.1358, Test Acc 65.95%\n",
            "Fold 5, Epoch 4: \n",
            "Train Loss 1.1436, Train Acc 65.83%, Test Loss 1.1049, Test Acc 66.95%\n",
            "Fold 5, Epoch 5: \n",
            "Train Loss 1.1155, Train Acc 66.70%, Test Loss 1.0747, Test Acc 67.79%\n",
            "Fold 5, Epoch 6: \n",
            "Train Loss 1.0912, Train Acc 67.38%, Test Loss 1.0521, Test Acc 68.44%\n",
            "Fold 5, Epoch 7: \n",
            "Train Loss 1.0722, Train Acc 67.94%, Test Loss 1.0333, Test Acc 69.07%\n",
            "Fold 5, Epoch 8: \n",
            "Train Loss 1.0554, Train Acc 68.53%, Test Loss 1.0129, Test Acc 69.78%\n",
            "Fold 5, Epoch 9: \n",
            "Train Loss 1.0382, Train Acc 69.13%, Test Loss 0.9911, Test Acc 70.43%\n",
            "Fold 5, Epoch 10: \n",
            "Train Loss 1.0196, Train Acc 69.68%, Test Loss 0.9708, Test Acc 71.00%\n",
            "Fold 5, Epoch 11: \n",
            "Train Loss 1.0027, Train Acc 70.18%, Test Loss 0.9537, Test Acc 71.51%\n",
            "Fold 5, Epoch 12: \n",
            "Train Loss 0.9888, Train Acc 70.60%, Test Loss 0.9395, Test Acc 71.90%\n",
            "Fold 5, Epoch 13: \n",
            "Train Loss 0.9777, Train Acc 70.92%, Test Loss 0.9278, Test Acc 72.23%\n",
            "Fold 5, Epoch 14: \n",
            "Train Loss 0.9676, Train Acc 71.22%, Test Loss 0.9188, Test Acc 72.48%\n",
            "Fold 5, Epoch 15: \n",
            "Train Loss 0.9592, Train Acc 71.47%, Test Loss 0.9100, Test Acc 72.73%\n",
            "Fold 5, Epoch 16: \n",
            "Train Loss 0.9516, Train Acc 71.70%, Test Loss 0.9019, Test Acc 72.99%\n",
            "Fold 5, Epoch 17: \n",
            "Train Loss 0.9440, Train Acc 71.92%, Test Loss 0.8937, Test Acc 73.24%\n",
            "Fold 5, Epoch 18: \n",
            "Train Loss 0.9365, Train Acc 72.16%, Test Loss 0.8862, Test Acc 73.44%\n",
            "Fold 5, Epoch 19: \n",
            "Train Loss 0.9307, Train Acc 72.35%, Test Loss 0.8794, Test Acc 73.65%\n",
            "Fold 5, Epoch 20: \n",
            "Train Loss 0.9246, Train Acc 72.53%, Test Loss 0.8730, Test Acc 73.83%\n",
            "Fold 5, Epoch 21: \n",
            "Train Loss 0.9191, Train Acc 72.70%, Test Loss 0.8665, Test Acc 74.03%\n",
            "Fold 5, Epoch 22: \n",
            "Train Loss 0.9137, Train Acc 72.86%, Test Loss 0.8606, Test Acc 74.21%\n",
            "Fold 5, Epoch 23: \n",
            "Train Loss 0.9087, Train Acc 73.02%, Test Loss 0.8549, Test Acc 74.36%\n",
            "Fold 5, Epoch 24: \n",
            "Train Loss 0.9038, Train Acc 73.17%, Test Loss 0.8493, Test Acc 74.53%\n",
            "Fold 5, Epoch 25: \n",
            "Train Loss 0.8994, Train Acc 73.30%, Test Loss 0.8440, Test Acc 74.68%\n",
            "Fold 5, Epoch 26: \n",
            "Train Loss 0.8952, Train Acc 73.43%, Test Loss 0.8392, Test Acc 74.81%\n",
            "Fold 5, Epoch 27: \n",
            "Train Loss 0.8911, Train Acc 73.53%, Test Loss 0.8343, Test Acc 74.92%\n",
            "Fold 5, Epoch 28: \n",
            "Train Loss 0.8871, Train Acc 73.65%, Test Loss 0.8304, Test Acc 75.03%\n",
            "Fold 5, Epoch 29: \n",
            "Train Loss 0.8833, Train Acc 73.75%, Test Loss 0.8260, Test Acc 75.13%\n",
            "Fold 5, Epoch 30: \n",
            "Train Loss 0.8796, Train Acc 73.84%, Test Loss 0.8225, Test Acc 75.20%\n",
            "Fold 5, Epoch 31: \n",
            "Train Loss 0.8767, Train Acc 73.91%, Test Loss 0.8191, Test Acc 75.27%\n",
            "Fold 5, Epoch 32: \n",
            "Train Loss 0.8728, Train Acc 74.01%, Test Loss 0.8157, Test Acc 75.37%\n",
            "Fold 5, Epoch 33: \n",
            "Train Loss 0.8708, Train Acc 74.06%, Test Loss 0.8126, Test Acc 75.43%\n",
            "Fold 5, Epoch 34: \n",
            "Train Loss 0.8679, Train Acc 74.13%, Test Loss 0.8100, Test Acc 75.48%\n",
            "Fold 5, Epoch 35: \n",
            "Train Loss 0.8653, Train Acc 74.19%, Test Loss 0.8074, Test Acc 75.55%\n",
            "Fold 5, Epoch 36: \n",
            "Train Loss 0.8631, Train Acc 74.24%, Test Loss 0.8049, Test Acc 75.60%\n",
            "Fold 5, Epoch 37: \n",
            "Train Loss 0.8603, Train Acc 74.31%, Test Loss 0.8028, Test Acc 75.65%\n",
            "Fold 5, Epoch 38: \n",
            "Train Loss 0.8583, Train Acc 74.36%, Test Loss 0.8007, Test Acc 75.71%\n",
            "Fold 5, Epoch 39: \n",
            "Train Loss 0.8565, Train Acc 74.40%, Test Loss 0.7988, Test Acc 75.73%\n",
            "Fold 5, Epoch 40: \n",
            "Train Loss 0.8547, Train Acc 74.45%, Test Loss 0.7967, Test Acc 75.79%\n",
            "Fold 5, Epoch 41: \n",
            "Train Loss 0.8522, Train Acc 74.51%, Test Loss 0.7949, Test Acc 75.84%\n",
            "Fold 5, Epoch 42: \n",
            "Train Loss 0.8503, Train Acc 74.55%, Test Loss 0.7933, Test Acc 75.87%\n",
            "Fold 5, Epoch 43: \n",
            "Train Loss 0.8487, Train Acc 74.59%, Test Loss 0.7918, Test Acc 75.90%\n",
            "Fold 5, Epoch 44: \n",
            "Train Loss 0.8475, Train Acc 74.62%, Test Loss 0.7899, Test Acc 75.94%\n",
            "Fold 5, Epoch 45: \n",
            "Train Loss 0.8458, Train Acc 74.66%, Test Loss 0.7881, Test Acc 75.98%\n",
            "Fold 5, Epoch 46: \n",
            "Train Loss 0.8443, Train Acc 74.69%, Test Loss 0.7867, Test Acc 76.01%\n",
            "Fold 5, Epoch 47: \n",
            "Train Loss 0.8429, Train Acc 74.72%, Test Loss 0.7850, Test Acc 76.05%\n",
            "Fold 5, Epoch 48: \n",
            "Train Loss 0.8413, Train Acc 74.76%, Test Loss 0.7835, Test Acc 76.08%\n",
            "Fold 5, Epoch 49: \n",
            "Train Loss 0.8397, Train Acc 74.79%, Test Loss 0.7818, Test Acc 76.11%\n",
            "Fold 5, Epoch 50: \n",
            "Train Loss 0.8384, Train Acc 74.81%, Test Loss 0.7803, Test Acc 76.14%\n",
            "Fold 5, Epoch 51: \n",
            "Train Loss 0.8373, Train Acc 74.84%, Test Loss 0.7788, Test Acc 76.17%\n",
            "Fold 5, Epoch 52: \n",
            "Train Loss 0.8355, Train Acc 74.87%, Test Loss 0.7771, Test Acc 76.21%\n",
            "Fold 5, Epoch 53: \n",
            "Train Loss 0.8341, Train Acc 74.90%, Test Loss 0.7757, Test Acc 76.22%\n",
            "Fold 5, Epoch 54: \n",
            "Train Loss 0.8331, Train Acc 74.92%, Test Loss 0.7743, Test Acc 76.25%\n",
            "Fold 5, Epoch 55: \n",
            "Train Loss 0.8311, Train Acc 74.96%, Test Loss 0.7729, Test Acc 76.27%\n",
            "Fold 5, Epoch 56: \n",
            "Train Loss 0.8302, Train Acc 74.98%, Test Loss 0.7713, Test Acc 76.31%\n",
            "Fold 5, Epoch 57: \n",
            "Train Loss 0.8289, Train Acc 74.99%, Test Loss 0.7699, Test Acc 76.32%\n",
            "Fold 5, Epoch 58: \n",
            "Train Loss 0.8276, Train Acc 75.02%, Test Loss 0.7689, Test Acc 76.35%\n",
            "Fold 5, Epoch 59: \n",
            "Train Loss 0.8262, Train Acc 75.05%, Test Loss 0.7675, Test Acc 76.36%\n",
            "Fold 5, Epoch 60: \n",
            "Train Loss 0.8252, Train Acc 75.06%, Test Loss 0.7661, Test Acc 76.38%\n",
            "Fold 5, Epoch 61: \n",
            "Train Loss 0.8236, Train Acc 75.09%, Test Loss 0.7651, Test Acc 76.40%\n",
            "Fold 5, Epoch 62: \n",
            "Train Loss 0.8223, Train Acc 75.11%, Test Loss 0.7638, Test Acc 76.42%\n",
            "Fold 5, Epoch 63: \n",
            "Train Loss 0.8219, Train Acc 75.11%, Test Loss 0.7628, Test Acc 76.44%\n",
            "Fold 5, Epoch 64: \n",
            "Train Loss 0.8203, Train Acc 75.14%, Test Loss 0.7616, Test Acc 76.46%\n",
            "Fold 5, Epoch 65: \n",
            "Train Loss 0.8191, Train Acc 75.17%, Test Loss 0.7606, Test Acc 76.47%\n",
            "Fold 5, Epoch 66: \n",
            "Train Loss 0.8178, Train Acc 75.20%, Test Loss 0.7600, Test Acc 76.48%\n",
            "Fold 5, Epoch 67: \n",
            "Train Loss 0.8173, Train Acc 75.20%, Test Loss 0.7592, Test Acc 76.49%\n",
            "Fold 5, Epoch 68: \n",
            "Train Loss 0.8162, Train Acc 75.22%, Test Loss 0.7586, Test Acc 76.51%\n",
            "Fold 5, Epoch 69: \n",
            "Train Loss 0.8155, Train Acc 75.24%, Test Loss 0.7576, Test Acc 76.53%\n",
            "Fold 5, Epoch 70: \n",
            "Train Loss 0.8149, Train Acc 75.25%, Test Loss 0.7571, Test Acc 76.54%\n",
            "Fold 5, Epoch 71: \n",
            "Train Loss 0.8136, Train Acc 75.28%, Test Loss 0.7567, Test Acc 76.55%\n",
            "Fold 5, Epoch 72: \n",
            "Train Loss 0.8129, Train Acc 75.29%, Test Loss 0.7561, Test Acc 76.56%\n",
            "Fold 5, Epoch 73: \n",
            "Train Loss 0.8123, Train Acc 75.31%, Test Loss 0.7555, Test Acc 76.58%\n",
            "Fold 5, Epoch 74: \n",
            "Train Loss 0.8119, Train Acc 75.31%, Test Loss 0.7552, Test Acc 76.58%\n",
            "Fold 5, Epoch 75: \n",
            "Train Loss 0.8113, Train Acc 75.33%, Test Loss 0.7544, Test Acc 76.60%\n",
            "Fold 5, Epoch 76: \n",
            "Train Loss 0.8110, Train Acc 75.34%, Test Loss 0.7540, Test Acc 76.61%\n",
            "Fold 5, Epoch 77: \n",
            "Train Loss 0.8098, Train Acc 75.37%, Test Loss 0.7536, Test Acc 76.62%\n",
            "Fold 5, Epoch 78: \n",
            "Train Loss 0.8090, Train Acc 75.38%, Test Loss 0.7531, Test Acc 76.64%\n",
            "Fold 5, Epoch 79: \n",
            "Train Loss 0.8084, Train Acc 75.40%, Test Loss 0.7526, Test Acc 76.65%\n",
            "Fold 5, Epoch 80: \n",
            "Train Loss 0.8082, Train Acc 75.39%, Test Loss 0.7524, Test Acc 76.65%\n",
            "Fold 5, Epoch 81: \n",
            "Train Loss 0.8078, Train Acc 75.40%, Test Loss 0.7517, Test Acc 76.67%\n",
            "Fold 5, Epoch 82: \n",
            "Train Loss 0.8073, Train Acc 75.42%, Test Loss 0.7515, Test Acc 76.67%\n",
            "Fold 5, Epoch 83: \n",
            "Train Loss 0.8065, Train Acc 75.43%, Test Loss 0.7511, Test Acc 76.68%\n",
            "Fold 5, Epoch 84: \n",
            "Train Loss 0.8056, Train Acc 75.46%, Test Loss 0.7507, Test Acc 76.69%\n",
            "Fold 5, Epoch 85: \n",
            "Train Loss 0.8054, Train Acc 75.46%, Test Loss 0.7503, Test Acc 76.70%\n",
            "Fold 5, Epoch 86: \n",
            "Train Loss 0.8050, Train Acc 75.47%, Test Loss 0.7499, Test Acc 76.71%\n",
            "Fold 5, Epoch 87: \n",
            "Train Loss 0.8043, Train Acc 75.49%, Test Loss 0.7497, Test Acc 76.71%\n",
            "Fold 5, Epoch 88: \n",
            "Train Loss 0.8045, Train Acc 75.48%, Test Loss 0.7493, Test Acc 76.72%\n",
            "Fold 5, Epoch 89: \n",
            "Train Loss 0.8039, Train Acc 75.50%, Test Loss 0.7488, Test Acc 76.73%\n",
            "Fold 5, Epoch 90: \n",
            "Train Loss 0.8035, Train Acc 75.51%, Test Loss 0.7486, Test Acc 76.74%\n",
            "Fold 5, Epoch 91: \n",
            "Train Loss 0.8030, Train Acc 75.51%, Test Loss 0.7484, Test Acc 76.75%\n",
            "Fold 5, Epoch 92: \n",
            "Train Loss 0.8026, Train Acc 75.52%, Test Loss 0.7479, Test Acc 76.76%\n",
            "Fold 5, Epoch 93: \n",
            "Train Loss 0.8018, Train Acc 75.55%, Test Loss 0.7477, Test Acc 76.77%\n",
            "Fold 5, Epoch 94: \n",
            "Train Loss 0.8015, Train Acc 75.55%, Test Loss 0.7471, Test Acc 76.78%\n",
            "Fold 5, Epoch 95: \n",
            "Train Loss 0.8012, Train Acc 75.56%, Test Loss 0.7470, Test Acc 76.78%\n",
            "Fold 5, Epoch 96: \n",
            "Train Loss 0.8010, Train Acc 75.56%, Test Loss 0.7468, Test Acc 76.79%\n",
            "Fold 5, Epoch 97: \n",
            "Train Loss 0.8007, Train Acc 75.58%, Test Loss 0.7463, Test Acc 76.80%\n",
            "Fold 5, Epoch 98: \n",
            "Train Loss 0.8003, Train Acc 75.58%, Test Loss 0.7461, Test Acc 76.79%\n",
            "Fold 5, Epoch 99: \n",
            "Train Loss 0.8004, Train Acc 75.58%, Test Loss 0.7457, Test Acc 76.81%\n",
            "Fold 5, Epoch 100: \n",
            "Train Loss 0.7999, Train Acc 75.59%, Test Loss 0.7457, Test Acc 76.80%\n",
            "\n",
            "Training report saved at: training_report.csv\n",
            "Best Test Accuracy across folds: 76.93%\n",
            "Best model saved at: best_wordle_model.pth\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        csv_path=\"/content/turkish1000000.csv\",\n",
        "        epochs=100,\n",
        "        batch_size=256,\n",
        "        lr=1e-4,\n",
        "        shuffle=True\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}