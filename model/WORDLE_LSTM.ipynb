{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OjwgkXIrHoM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJqFpeAgrSBw"
      },
      "source": [
        "LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIAyyVJ3rUPo"
      },
      "outputs": [],
      "source": [
        "class WordleLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based neural network for predicting Wordle words.\n",
        "\n",
        "    Architecture rationale:\n",
        "    - Embedding layer: Converts integer-encoded letters into dense vectors.\n",
        "      This allows the model to learn semantic similarities between letters or feedback.\n",
        "    - LSTM: Captures sequential dependencies between consecutive guesses within a game.\n",
        "      Since Wordle guesses follow a sequential pattern (later guesses depend on earlier feedback),\n",
        "      LSTM is suitable for learning these temporal relationships.\n",
        "    - Dropout layers: Reduce overfitting by randomly dropping units during training.\n",
        "    - Fully connected layers: Transform LSTM hidden states into predictions for each letter.\n",
        "      The output dimension is 5 letters * 29 possible classes (vocab_size).\n",
        "    - Packed sequences: Handle variable-length games efficiently.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=29,\n",
        "        embedding_dim=16,\n",
        "        input_dim=10,\n",
        "        hidden_dim=128,\n",
        "        num_layers=2,\n",
        "        dropout=0.3\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.output_dim = 5 * vocab_size  # 5 Letter  * 29 class = 145\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm_input_dim = input_dim * embedding_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            self.lstm_input_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.post_lstm_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, self.output_dim)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "\n",
        "        batch_size, seq_len, fifteen_dim = x.size()\n",
        "\n",
        "        x_embed = self.embedding(x)\n",
        "        x_embed = x_embed.view(batch_size, seq_len, -1)\n",
        "\n",
        "        packed_x = nn.utils.rnn.pack_padded_sequence(\n",
        "            x_embed,\n",
        "            lengths.cpu(),\n",
        "            batch_first=True,\n",
        "            enforce_sorted=False\n",
        "        )\n",
        "        packed_out, (h, c) = self.lstm(packed_x)\n",
        "        out, out_lengths = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
        "\n",
        "        out = self.post_lstm_dropout(out)\n",
        "\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.post_lstm_dropout(out)\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        b_size, seq_len, _ = out.size()\n",
        "        out = out.view(b_size, seq_len, 5, -1)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAvNETa0rsKp"
      },
      "source": [
        "TRAIN AND TEST FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1We-K-VErpNy"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    \"\"\"\n",
        "    Trains the given model for one epoch using the provided DataLoader.\n",
        "\n",
        "    This function performs a full training loop:\n",
        "    - Iterates over all batches in the DataLoader\n",
        "    - Moves data to the target device (CPU/GPU)\n",
        "    - Computes model predictions (logits)\n",
        "    - Calculates loss and backpropagates gradients\n",
        "    - Updates model weights using the optimizer\n",
        "    - Tracks average loss and overall accuracy\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be trained\n",
        "        dataloader (torch.utils.data.DataLoader): Provides training batches\n",
        "        optimizer (torch.optim.Optimizer): Optimization algorithm (e.g., Adam, SGD)\n",
        "        criterion (torch.nn.Module): Loss function (e.g., CrossEntropyLoss)\n",
        "        device (torch.device): Target device (\"cpu\" or \"cuda\")\n",
        "\n",
        "    Returns:\n",
        "        avg_loss (float): Average loss over all batches\n",
        "        accuracy (float): Overall prediction accuracy (in %)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for X, Y, lengths in dataloader:\n",
        "        X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(X, lengths)\n",
        "        logits_flat = logits.view(-1, 29)\n",
        "        Y_flat = Y.view(-1)\n",
        "\n",
        "        loss = criterion(logits_flat, Y_flat)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(logits_flat, dim=1)\n",
        "        correct_predictions += (predicted == Y_flat).sum().item()\n",
        "        total_predictions += Y_flat.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = 100.0 * correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVvtwRaxrrdg"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluates the given model on a validation or test dataset.\n",
        "\n",
        "    This function performs an evaluation loop:\n",
        "    - Sets the model to evaluation mode (disables dropout, batchnorm updates)\n",
        "    - Iterates over all batches in the DataLoader without computing gradients\n",
        "    - Computes model predictions (logits)\n",
        "    - Calculates loss and overall accuracy\n",
        "    - Returns average loss and accuracy across the dataset\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to be evaluated\n",
        "        dataloader (torch.utils.data.DataLoader): Provides test/validation batches\n",
        "        criterion (torch.nn.Module): Loss function (e.g., CrossEntropyLoss)\n",
        "        device (torch.device): Target device (\"cpu\" or \"cuda\")\n",
        "\n",
        "    Returns:\n",
        "        avg_loss (float): Average loss over all batches\n",
        "        accuracy (float): Overall prediction accuracy (in %)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, Y, lengths in dataloader:\n",
        "            X, Y, lengths = X.to(device), Y.to(device), lengths.to(device)\n",
        "\n",
        "            logits = model(X, lengths)\n",
        "            logits_flat = logits.view(-1, 29)\n",
        "            Y_flat = Y.view(-1)\n",
        "\n",
        "            loss = criterion(logits_flat, Y_flat)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(logits_flat, dim=1)\n",
        "            correct_predictions += (predicted == Y_flat).sum().item()\n",
        "            total_predictions += Y_flat.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0.0\n",
        "    accuracy = 100.0 * correct_predictions / total_predictions if total_predictions > 0 else 0.0\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pczJBhSwrzoU"
      },
      "source": [
        "Sequence Dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaAjeQWjsIYu"
      },
      "outputs": [],
      "source": [
        "class WordleSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset for handling Wordle gameplay data in sequence format.\n",
        "    - Each game (identified by 'gameid') contains multiple attempts (rows).\n",
        "    - Each attempt has encoded features for guesses and feedback.\n",
        "    - The dataset groups and converts them into sequential tensors for model training.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_frame):\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequences = []\n",
        "        self.labels = []\n",
        "        self.lengths = []\n",
        "        # Group all attempts by game ID (each game is treated as one sequence)\n",
        "        grouped = data_frame.groupby(\"gameid\")\n",
        "        # Process each game sequence separately\n",
        "        for gameid, group in grouped:\n",
        "            group_sorted = group.sort_values(by=\"attempt_index\")\n",
        "\n",
        "            # Input vector X:\n",
        "            # - pg1..pg5: previous guessed letters\n",
        "            # - l1..l5: feedback for each letter (e.g., correct/wrong position)\n",
        "            # Combined feature dimension = 10 per timestep\n",
        "            X = group_sorted[[\n",
        "                \"pg1\", \"pg2\", \"pg3\", \"pg4\", \"pg5\",\n",
        "                \"l1\", \"l2\", \"l3\", \"l4\", \"l5\"\n",
        "                        ]].values\n",
        "\n",
        "            # Target vector Y:\n",
        "            # - t1..t5: target word (true letters encoded as integers)\n",
        "            Y = group_sorted[[\"t1\", \"t2\", \"t3\", \"t4\", \"t5\"]].values\n",
        "\n",
        "            X_tensor = torch.tensor(X, dtype=torch.long)\n",
        "            Y_tensor = torch.tensor(Y, dtype=torch.long)\n",
        "\n",
        "            self.sequences.append(X_tensor)\n",
        "            self.labels.append(Y_tensor)\n",
        "            self.lengths.append(X_tensor.size(0))\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the number of games in the dataset\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return one game sequence (X, Y, length)\n",
        "        return self.sequences[idx], self.labels[idx], self.lengths[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader.\n",
        "    Pads sequences of different lengths so they can form uniform batches.\n",
        "    Each batch is a list of tuples: (X_seq, Y_seq, seq_len)\n",
        "    \"\"\"\n",
        "    sequences = [item[0] for item in batch]\n",
        "    labels = [item[1] for item in batch]\n",
        "    lengths = [item[2] for item in batch]\n",
        "    # Pad sequences with zeros to match the length of the longest sequence in the batch\n",
        "    # This allows variable-length games to be trained together efficiently\n",
        "    padded_X = pad_sequence(sequences, batch_first=True)\n",
        "    padded_Y = pad_sequence(labels, batch_first=True)\n",
        "    # Convert lengths to a tensor (used later for masking or packing)\n",
        "    lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
        "\n",
        "    return padded_X, padded_Y, lengths_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwdBiR2jr4BU"
      },
      "outputs": [],
      "source": [
        "def prepare_input_dataLoader(csv_path,batch_size,test_ratio,shuffle):\n",
        "    # Read the CSV file containing the dataset\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    if df.isnull().any().any():\n",
        "        raise ValueError(\"CSV dosyasında eksik değerler var!\")\n",
        "    # Get all unique game IDs (each game corresponds to a sequence of guesses)\n",
        "    all_gameids = df['gameid'].unique()\n",
        "    # Split the game IDs into training and test sets\n",
        "    train_ids, test_ids = train_test_split(all_gameids, test_size=test_ratio, shuffle=shuffle)\n",
        "    train_df = df[df['gameid'].isin(train_ids)]\n",
        "    test_df = df[df['gameid'].isin(test_ids)]\n",
        "    # Create dataset objects for training and testing\n",
        "    train_dataset = WordleSequenceDataset(train_df)\n",
        "    test_dataset = WordleSequenceDataset(test_df)\n",
        "    # Create DataLoaders for batching and shuffling the data\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDVTVRmqr8kx"
      },
      "outputs": [],
      "source": [
        "def main(csv_path=\"\",\n",
        "         epochs=1,\n",
        "         batch_size=1,\n",
        "         lr=1e-3,\n",
        "         test_ratio=0.2,\n",
        "         shuffle=True):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Running on: {device}\")\n",
        "    train_loader, test_loader = prepare_input_dataLoader(csv_path=csv_path,batch_size=batch_size,test_ratio=test_ratio,shuffle=shuffle)\n",
        "    # Model, optimizer, loss\n",
        "    model = WordleLSTM(\n",
        "        vocab_size=29,      # letter numbers in alphabet\n",
        "        embedding_dim=16,   # Embedding dimension\n",
        "        input_dim=10,       # input dimension\n",
        "        hidden_dim=256,     # LSTM hidden dimension\n",
        "        num_layers=4,       # LSTM layer number\n",
        "        dropout=0.3         # Drop out\n",
        "    ).to(device)\n",
        "    lstm_optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    lstm_criterion = nn.CrossEntropyLoss()\n",
        "    #kfold_result = kfold_training(csv_path, model, k=5, batch_size=batch_size, num_epochs=epoch, device=device, random_state=42)\n",
        "    for epoch in range(epochs):\n",
        "        lstm_train_loss, lstm_train_acc = train(model, train_loader, lstm_optimizer, lstm_criterion, device)\n",
        "        lstm_test_loss, lstm_test_acc = test(model, test_loader, lstm_criterion, device)\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
        "        print(f\"LSTM  Train Loss: {lstm_train_loss:.4f}, Train Acc: {lstm_train_acc:.2f}%\")\n",
        "        print(f\"LTSM  Test  Loss: {lstm_test_loss:.4f}, Test  Acc: {lstm_test_acc:.2f}%\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"tr_LSTMmodel_100epoch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yFSH1-e41s6"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        csv_path=\"/content/turkishgamelog1000000.csv\",\n",
        "        epochs=100,\n",
        "        batch_size=256,\n",
        "        lr=1e-4,\n",
        "        test_ratio=0.2,\n",
        "        shuffle=True\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
